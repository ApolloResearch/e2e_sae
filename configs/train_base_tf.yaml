seed: 0
model:
  n_layers: 2
  d_model: 16
  n_ctx: 32
  d_head: 4
  model_name: "dev_custom"
  n_heads: 5
  d_mlp: null
  act_fn: 'gelu'
  d_vocab: 19
  eps: 1e-5
  use_attn_result: False
  use_attn_scale: True
  use_split_qkv_input: False
  use_hook_mlp_in: False
  use_attn_in: False
  use_local_attn: False
  original_architecture: null
  from_checkpoint: False
  checkpoint_index: null
  checkpoint_label_type: null
  checkpoint_value: null
  tokenizer_name: "gpt2"
  window_size: null
  attn_types: null
  init_mode: "gpt2"
  normalization_type: "LN"
  device: null
  n_devices: 1
  attention_dir: "causal"
  attn_only: False
  seed: 0
  initializer_range: -1.0
  init_weights: True
  scale_attn_by_inverse_layer_idx: False
  positional_embedding_type: "standard"
  final_rms: False
  d_vocab_out: 19
  parallel_attn_mlp: False
  rotary_dim: null
  n_params: null
  use_hook_tokens: False
  gated_mlp: False
  default_prepend_bos: True
  dtype: null
  tokenizer_prepends_bos: null
  post_embedding_ln: False
  rotary_adjacent_pairs: False
pretrained_model_type: null
train:
  num_epochs: 1
  batch_size: 32
  lr: 0.001
  seed: 0
  momentum: 0.0
  max_grad_norm: null
  weight_decay: null
  optimizer_name: "Adam"
  device: null
  warmup_steps: 0
  save_every: null
  save_dir: null
  wandb: False
  wandb_project_name: null
  print_every: 50
  max_steps: 1000
wandb:
  project: tf-sae
  entity: lee_sharkey
