{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1fbd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections.abc import Generator\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtyping import TensorType\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "@dataclass\n",
    "class ToyArgs:\n",
    "    device: Union[str, Any] = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tied_ae: bool = False\n",
    "    seed: int = 103\n",
    "    learned_dict_ratio: float = 1.0\n",
    "    output_folder: str = \"outputs\"\n",
    "    # dtype: torch.dtype = torch.float32\n",
    "    activation_dim: int = 32\n",
    "    feature_prob_decay: float = 0.99\n",
    "    feature_num_nonzero: int = 8\n",
    "    correlated_components: bool = False\n",
    "    n_ground_truth_components: int = 128\n",
    "    batch_size: int = 4_096\n",
    "    lr: float = 4e-4\n",
    "    epochs: int = 300_000\n",
    "    n_components_dictionary: int = 256\n",
    "    n_components_dictionary_trans: int = 256\n",
    "    l1_alpha: float = 5e-3\n",
    "    use_topk: bool = False\n",
    "    topk: list[int] = field(\n",
    "        default_factory=lambda: [\n",
    "            1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16\n",
    "            # 7, 8, 9, 10, 11\n",
    "        ]\n",
    "    )\n",
    "    norm_output: bool = False\n",
    "    lp_alphas: list[float] = field(\n",
    "        # default_factory=lambda: [\n",
    "        #     1e-4,\n",
    "        #     1.8e-4,\n",
    "        #     3e-4,\n",
    "        #     5.6e-4,\n",
    "        #     1e-3,\n",
    "        #     1.8e-3,\n",
    "        #     3e-3,\n",
    "        #     5.6e-3,\n",
    "        #     1e-2,\n",
    "        #     1.8e-2,\n",
    "        #     3e-2,\n",
    "        #     5.6e-2,\n",
    "        #     1e-1,\n",
    "        #     1.8e-1,\n",
    "        #     3e-1,\n",
    "        #     5.6e-1,\n",
    "        #     1,\n",
    "        #     1.8,\n",
    "        #     3,\n",
    "        # ]\n",
    "        # default_factory=lambda: [\n",
    "        #     5.6,\n",
    "        #     1e1,\n",
    "        #     1.8e1,\n",
    "        #     3e1,\n",
    "        #     5.6e1,\n",
    "        #     1e2,\n",
    "        #     1.8e2,\n",
    "        #     3e2,\n",
    "        # ]\n",
    "        # default_factory=lambda: [ # fine\n",
    "        #     1e-2,\n",
    "        #     1.8e-2,\n",
    "        #     3e-2,\n",
    "        #     5.6e-2,\n",
    "        #     1e-1,\n",
    "        #     1.8e-1,\n",
    "        #     3e-1,\n",
    "        #     5.6e-1,\n",
    "        #     1,\n",
    "        #     1.8,\n",
    "        #     3,\n",
    "        # ]\n",
    "        # default_factory=lambda: [ # fine extension\n",
    "        #     5.6,\n",
    "        #     10,\n",
    "        #     18,\n",
    "        # ]\n",
    "        default_factory=lambda: [\n",
    "            3,\n",
    "        ]\n",
    "    )\n",
    "    # lp_alphas: list[float] = field(\n",
    "    #     default_factory=lambda: [\n",
    "    #         1e-7,\n",
    "    #         3e-7,\n",
    "    #         1e-6,\n",
    "    #         3e-6,\n",
    "    #         1e-5,\n",
    "    #         3e-5,\n",
    "    #         1e-4,\n",
    "    #         3e-4,\n",
    "    #         1e-3,\n",
    "    #         3e-3,\n",
    "    #         1e-2,\n",
    "    #         3e-2,\n",
    "    #         1e-1,\n",
    "    #         3e-1,\n",
    "    #         1,\n",
    "    #         3,\n",
    "    #     ]\n",
    "    # )\n",
    "    # lp_alphas: list[float] = field(\n",
    "    #     default_factory=lambda: [\n",
    "    #         1\n",
    "    #     ]\n",
    "    # )\n",
    "    p_values: list[float] = field(\n",
    "        # default_factory=lambda: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1]\n",
    "        # default_factory=lambda: [0.1, 0.4, 0.7, 1,]\n",
    "        default_factory=lambda: [1,]\n",
    "    )\n",
    "    anneal: bool = True\n",
    "    loss_fn: str =  \"lp^p\" # options for sparsity penalty: lp_norm, lp^p, log, gated, None\n",
    "    eps: list[float] = field(\n",
    "        default_factory=lambda: [\n",
    "            0.01, 0.1\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    def ps(self):\n",
    "        if self.use_topk:\n",
    "            return self.topk\n",
    "        elif self.loss_fn==\"log\":\n",
    "            return self.eps\n",
    "        else:\n",
    "            return self.p_values\n",
    "        \n",
    "\n",
    "@dataclass\n",
    "class RandomDatasetGenerator(Generator):\n",
    "    activation_dim: int\n",
    "    n_ground_truth_components: int\n",
    "    batch_size: int\n",
    "    feature_num_nonzero: int\n",
    "    feature_prob_decay: float\n",
    "    correlated: bool\n",
    "    device: Union[torch.device, str]\n",
    "\n",
    "    feats: Optional[TensorType[\"n_ground_truth_components\", \"activation_dim\"]] = None\n",
    "    generated_so_far: int = 0\n",
    "\n",
    "    frac_nonzero: float = field(init=False)\n",
    "    decay: TensorType[\"n_ground_truth_components\"] = field(init=False)\n",
    "    corr_matrix: Optional[\n",
    "        TensorType[\"n_ground_truth_components\", \"n_ground_truth_components\"]\n",
    "    ] = field(init=False)\n",
    "    component_probs: Optional[TensorType[\"n_ground_truth_components\"]] = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.frac_nonzero = self.feature_num_nonzero / self.n_ground_truth_components\n",
    "\n",
    "        # Define the probabilities of each component being included in the data\n",
    "        self.decay = torch.tensor(\n",
    "            [self.feature_prob_decay**i for i in range(self.n_ground_truth_components)]\n",
    "        ).to(self.device)  # FIXME: 1 / i\n",
    "\n",
    "        self.component_probs = self.decay * self.frac_nonzero  # Only if non-correlated\n",
    "        if self.feats is None:\n",
    "            self.feats = generate_rand_feats(\n",
    "                self.activation_dim,\n",
    "                self.n_ground_truth_components,\n",
    "                device=self.device,\n",
    "            )\n",
    "\n",
    "    def send(self, ignored_arg: Any) -> TensorType[\"dataset_size\", \"activation_dim\"]:\n",
    "        torch.manual_seed(self.generated_so_far)  # Set a deterministic seed for reproducibility\n",
    "        self.generated_so_far += 1\n",
    "\n",
    "        # Assuming generate_rand_dataset is your data generation function\n",
    "        _, ground_truth, data = generate_rand_dataset(\n",
    "            self.n_ground_truth_components,\n",
    "            self.batch_size,\n",
    "            self.component_probs,\n",
    "            self.feats,\n",
    "            self.device,\n",
    "        )\n",
    "        return ground_truth, data\n",
    "\n",
    "    def throw(self, type: Any = None, value: Any = None, traceback: Any = None) -> None:\n",
    "        raise StopIteration\n",
    "\n",
    "def generate_rand_dataset(\n",
    "    n_ground_truth_components: int,  #\n",
    "    dataset_size: int,\n",
    "    feature_probs: TensorType[\"n_ground_truth_components\"],\n",
    "    feats: TensorType[\"n_ground_truth_components\", \"activation_dim\"],\n",
    "    device: Union[torch.device, str],\n",
    ") -> tuple[\n",
    "    TensorType[\"n_ground_truth_components\", \"activation_dim\"],\n",
    "    TensorType[\"dataset_size\", \"n_ground_truth_components\"],\n",
    "    TensorType[\"dataset_size\", \"activation_dim\"],\n",
    "]:\n",
    "    # generate random feature strengths\n",
    "    feature_strengths = torch.rand((dataset_size, n_ground_truth_components), device=device)\n",
    "    # only some features are activated, chosen at random\n",
    "    dataset_thresh = torch.rand(dataset_size, n_ground_truth_components, device=device)\n",
    "    data_zero = torch.zeros_like(dataset_thresh, device=device)\n",
    "\n",
    "    dataset_codes = torch.where(\n",
    "        dataset_thresh <= feature_probs,\n",
    "        feature_strengths,\n",
    "        data_zero,\n",
    "    )  # dim: dataset_size x n_ground_truth_components\n",
    "\n",
    "    dataset = dataset_codes @ feats\n",
    "\n",
    "    return feats, dataset_codes, dataset\n",
    "\n",
    "def generate_rand_feats(\n",
    "    feat_dim: int,\n",
    "    num_feats: int,\n",
    "    device: Union[torch.device, str],\n",
    ") -> TensorType[\"n_ground_truth_components\", \"activation_dim\"]:\n",
    "    data_path = os.path.join(os.getcwd(), \"data\")\n",
    "    data_filename = os.path.join(data_path, f\"feats_{feat_dim}_{num_feats}.npy\")\n",
    "\n",
    "    feats = np.random.multivariate_normal(np.zeros(feat_dim), np.eye(feat_dim), size=num_feats)\n",
    "    feats = feats.T / np.linalg.norm(feats, axis=1)\n",
    "    feats = feats.T\n",
    "\n",
    "    feats_tensor = torch.from_numpy(feats).to(device).float()\n",
    "    return feats_tensor\n",
    "\n",
    "# AutoEncoder Definition\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, activation_size, n_dict_components):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(nn.Linear(activation_size, n_dict_components), nn.ReLU())\n",
    "        self.decoder = nn.Linear(n_dict_components, activation_size, bias=False)\n",
    "\n",
    "        # Initialize the decoder weights orthogonally\n",
    "        nn.init.orthogonal_(self.decoder.weight)\n",
    "\n",
    "    def forward(self, x, topk=None, norm_output=False):\n",
    "        c = self.encoder(x)\n",
    "        if topk is not None:\n",
    "            _, indices = torch.topk(c, topk, dim=-1, largest=True, sorted=False)\n",
    "            zero_tensor = torch.zeros_like(c)\n",
    "            zero_tensor.scatter_(-1, indices, c.gather(-1, indices))\n",
    "            c = zero_tensor\n",
    "\n",
    "        # Apply unit norm constraint to the decoder weights\n",
    "        self.decoder.weight.data = nn.functional.normalize(self.decoder.weight.data, dim=0)\n",
    "\n",
    "        x_hat = self.decoder(c)\n",
    "        \n",
    "        if norm_output:            \n",
    "            x_hat_norm = torch.norm(x_hat, p=2, dim=1, keepdim=True)\n",
    "            # If the norm is 0, we can't divide by it\n",
    "            x_hat = x_hat / torch.clamp(x_hat_norm, min=1e-8)\n",
    "            c = c / torch.clamp(x_hat_norm, min=1e-8)\n",
    "            \n",
    "        return x_hat, c\n",
    "\n",
    "    def get_dictionary(self):\n",
    "        self.decoder.weight.data = nn.functional.normalize(self.decoder.weight.data, dim=0)\n",
    "        return self.decoder.weight\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "def cosine_sim(\n",
    "    vecs1: Union[torch.Tensor, torch.nn.parameter.Parameter, npt.NDArray],\n",
    "    vecs2: Union[torch.Tensor, torch.nn.parameter.Parameter, npt.NDArray],\n",
    ") -> np.ndarray:\n",
    "    vecs = [vecs1, vecs2]\n",
    "    for i in range(len(vecs)):\n",
    "        if not isinstance(vecs[i], np.ndarray):\n",
    "            vecs[i] = vecs[i].detach().cpu().numpy()  # type: ignore\n",
    "    vecs1, vecs2 = vecs\n",
    "    normalize = lambda v: (v.T / np.linalg.norm(v, axis=1)).T\n",
    "    vecs1_norm = normalize(vecs1)\n",
    "    vecs2_norm = normalize(vecs2)\n",
    "\n",
    "    return vecs1_norm @ vecs2_norm.T\n",
    "\n",
    "def mean_max_cosine_similarity(ground_truth_features, learned_dictionary, debug=False):\n",
    "    # Calculate cosine similarity between all pairs of ground truth and learned features\n",
    "    cos_sim = cosine_sim(ground_truth_features, learned_dictionary)\n",
    "    # Find the maximum cosine similarity for each ground truth feature, then average\n",
    "    mmcs = cos_sim.max(axis=1).mean()\n",
    "    return mmcs\n",
    "\n",
    "def calculate_mmcs(auto_encoder, ground_truth_features):\n",
    "    learned_dictionary = auto_encoder.decoder.weight.data.t()\n",
    "    with torch.no_grad():\n",
    "        mmcs = mean_max_cosine_similarity(\n",
    "            ground_truth_features.to(auto_encoder.device), learned_dictionary\n",
    "        )\n",
    "    return mmcs\n",
    "\n",
    "def get_alive_neurons(auto_encoder, data_generator, n_batches=10):\n",
    "    \"\"\"\n",
    "    :param result_dict: dictionary containing the results of a single run\n",
    "    :return: number of dead neurons\n",
    "\n",
    "    Estimates the number of dead neurons in the network by running a few batches of data through the network and\n",
    "    calculating the mean activation of each neuron. If the mean activation is 0 for a neuron, it is considered dead.\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    for i in range(n_batches):\n",
    "        ground_truth, batch = next(data_generator)\n",
    "        x_hat, c = auto_encoder(\n",
    "            batch\n",
    "        )  # x_hat: (batch_size, activation_dim), c: (batch_size, n_dict_components)\n",
    "        outputs.append(c)\n",
    "    outputs = torch.cat(outputs)  # (n_batches * batch_size, n_dict_components)\n",
    "    mean_activations = outputs.mean(\n",
    "        dim=0\n",
    "    )  # (n_dict_components), c is after the ReLU, no need to take abs\n",
    "    alive_neurons = mean_activations > 0\n",
    "    return alive_neurons\n",
    "\n",
    "def zero_out_except_topk(tensor, topk):\n",
    "    # Keep only the topk values, set others to zero\n",
    "    _, indices = torch.topk(tensor, topk, dim=-1, largest=True, sorted=False)\n",
    "    zero_tensor = torch.zeros_like(tensor)\n",
    "    zero_tensor.scatter_(-1, indices, tensor.gather(-1, indices))\n",
    "    return zero_tensor\n",
    "\n",
    "def smooth_log(x, eps):\n",
    "    \"\"\"\n",
    "    Element-wise apply a smoothed log function to a torch tensor x such that:\n",
    "    smooth_log(x, eps) = 0 if x <= 0\n",
    "    smooth_log(x, eps) = log(x) - log(eps) + 1/2 if x > eps\n",
    "    smooth_log(x, eps) = 1/2 * x^2/eps^2 if 0 < x < eps\n",
    "    \"\"\"\n",
    "    # Tensor to hold the output values\n",
    "    y = torch.zeros_like(x)\n",
    "    \n",
    "    # Mask for values where x > eps\n",
    "    mask1 = x > eps\n",
    "    y[mask1] = torch.log(x[mask1]) - torch.log(eps) + 0.5\n",
    "    \n",
    "    # Mask for values where 0 < x <= eps\n",
    "    mask2 = (x > 0) & (x <= eps)\n",
    "    y[mask2] = 0.5 * (x[mask2] / eps) ** 2\n",
    "    \n",
    "    # Values where x <= 0 remain zero, as initialized\n",
    "    \n",
    "    return y\n",
    "\n",
    "def sparsity_loss_term(c,p, loss_fn):\n",
    "    if loss_fn == \"lp_norm\":\n",
    "        sparsity_term = torch.norm(c, p, dim=1).mean() / c.size(1)\n",
    "    elif loss_fn == \"lp^p\":\n",
    "        sparsity_term = torch.pow(c, p).sum(dim=-1).mean() / c.size(1)\n",
    "    elif loss_fn == \"log\":\n",
    "        sparsity_term = smooth_log(c, p).sum(dim=-1).mean() / c.size(1)\n",
    "    else:\n",
    "        sparsity_term = torch.tensor((0,), device=device)\n",
    "    return sparsity_term\n",
    "    \n",
    "\n",
    "def train_model(arg):\n",
    "    (\n",
    "        worker_id,\n",
    "        epochs,\n",
    "        p_id,\n",
    "        p,\n",
    "        lp_id,\n",
    "        lp_alpha,\n",
    "        ground_truth_features,\n",
    "        cfg_dict,\n",
    "        init_seed,\n",
    "        device,\n",
    "        run_num,\n",
    "    ) = arg\n",
    "    torch.cuda.set_device(device)  # Set the device for the process\n",
    "\n",
    "    data_generator = RandomDatasetGenerator(\n",
    "        activation_dim=cfg_dict[\"activation_dim\"],\n",
    "        n_ground_truth_components=cfg_dict[\"n_ground_truth_components\"],\n",
    "        batch_size=cfg_dict[\"batch_size\"],\n",
    "        feature_num_nonzero=cfg_dict[\"feature_num_nonzero\"],\n",
    "        feature_prob_decay=cfg_dict[\"feature_prob_decay\"],\n",
    "        correlated=cfg_dict[\"correlated_components\"],\n",
    "        device=device,\n",
    "        feats=(ground_truth_features),\n",
    "        generated_so_far=init_seed,\n",
    "    )\n",
    "\n",
    "    auto_encoder = AutoEncoder(cfg_dict[\"activation_dim\"], cfg_dict[\"n_components_dictionary\"]).to(\n",
    "        device\n",
    "    )\n",
    "\n",
    "    optimizer = optim.Adam(auto_encoder.parameters(), lr=cfg_dict[\"lr\"])\n",
    "\n",
    "    logs = []\n",
    "    \n",
    "    original_p = p\n",
    "    original_alpha = lp_alpha\n",
    "    \n",
    "    # create schedule to anneal p's\n",
    "    if cfg_dict[\"anneal\"]:\n",
    "        num_anneals = 10\n",
    "        # stored_sparsity_norms = []\n",
    "        anneal_ps = np.linspace(p, 0, num_anneals, endpoint=False)\n",
    "        # anneal_time\n",
    "        anneal_times = np.linspace(0,epochs/2, num_anneals, endpoint=False, dtype=int)\n",
    "        alpha_anneal_times = np.linspace(epochs/2,epochs, num_anneals, endpoint=False, dtype=int)\n",
    "        p_schedule = dict((t, p) for t,p in zip(anneal_times, anneal_ps))\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        ground_truth, batch = next(data_generator)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        topk = p if cfg_dict[\"use_topk\"] else None\n",
    "        x_hat, c = auto_encoder(batch, topk=topk, norm_output=cfg_dict[\"norm_output\"])\n",
    "        # also normalize input for accuracte reconstruction loss:\n",
    "        if cfg_dict[\"norm_output\"]:\n",
    "            x_hat = x_hat * torch.norm(batch, p=2, dim=1, keepdim=True)\n",
    "            c = x_hat * torch.norm(batch, p=2, dim=1, keepdim=True)\n",
    "\n",
    "        # Compute the reconstruction loss and L1 regularization\n",
    "        l_reconstruction = torch.nn.MSELoss()(batch, x_hat)\n",
    "        \n",
    "        sparsity_term = sparsity_loss_term(c,p,cfg_dict[\"loss_fn\"])\n",
    "        \n",
    "        l_lp = lp_alpha * sparsity_term\n",
    "        \n",
    "        if cfg_dict[\"anneal\"]:\n",
    "            if ep in p_schedule:\n",
    "                new_p = p_schedule[ep]\n",
    "                new_sparsity_term = sparsity_loss_term(c,new_p,cfg_dict[\"loss_fn\"])\n",
    "                new_alpha = l_lp / new_sparsity_term\n",
    "                p = new_p\n",
    "                print(f\"updating to p={new_p} and alpha={new_alpha}\")\n",
    "                \n",
    "                lp_alpha = new_alpha.detach().clone().item()\n",
    "                # if ep >0:\n",
    "                #     lp_alpha *= (0.5)**(1/10)\n",
    "            \n",
    "            if ep in alpha_anneal_times:\n",
    "                lp_alpha = float(lp_alpha * (0.1)**(1/10))\n",
    "            \n",
    "            \n",
    "\n",
    "        # Print the losses, mmcs, and current epoch\n",
    "        # mmcs = float(calculate_mmcs(auto_encoder, ground_truth_features).cpu().item())\n",
    "        # print(\"mmcs: \", type(float(mmcs)))\n",
    "        sparsity = (c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "        num_dead_features = (c == 0).float().mean(dim=0).sum().cpu().item()\n",
    "        \n",
    "        mmcs = calculate_mmcs(auto_encoder, ground_truth_features)\n",
    "\n",
    "        log_prefix = f\"{lp_alpha} L{p}\"\n",
    "        if cfg_dict[\"anneal\"]:\n",
    "            log_prefix = \"\"\n",
    "        if (ep+1) % 100 == 0:\n",
    "            wandb_log = {\n",
    "                # f\"{log_prefix} MMCS\": mmcs,\n",
    "                f\"{log_prefix} Sparsity\": sparsity,\n",
    "                f\"{log_prefix} Dead Features\": num_dead_features,\n",
    "                f\"{log_prefix} Reconstruction Loss\": l_reconstruction.detach().item(),\n",
    "                f\"{log_prefix} Sparsity Loss\": l_lp.detach().item(),\n",
    "                f\"{log_prefix} Sparsity Term\": sparsity_term.detach().item(),\n",
    "                f\"{log_prefix} MMCS\": mmcs,\n",
    "                f\"{log_prefix} p\": p,\n",
    "                \"Tokens\": ep * cfg_dict[\"batch_size\"],\n",
    "            }\n",
    "            logs.append(wandb_log)\n",
    "\n",
    "        # Compute the total loss\n",
    "        loss = l_reconstruction + l_lp\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Save model\n",
    "    save_name = f\"sae_l{original_p}_{original_alpha}\"\n",
    "    torch.save(auto_encoder, f\"/root/sparsify/trained_models/toy_saes{run_num}/{save_name}.pt\")\n",
    "    return logs\n",
    "\n",
    "\n",
    "def main():\n",
    "    cfg = ToyArgs()\n",
    "    cfg.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    run_num = cfg.seed\n",
    "\n",
    "    torch.manual_seed(cfg.seed)\n",
    "    np.random.seed(cfg.seed)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ground_truth_features = generate_rand_feats(\n",
    "        cfg.activation_dim,\n",
    "        cfg.n_ground_truth_components,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    mp.set_start_method(\"spawn\")\n",
    "\n",
    "    if not os.path.exists(f\"/root/sparsify/trained_models/toy_saes{run_num}\"):\n",
    "        os.makedirs(f\"/root/sparsify/trained_models/toy_saes{run_num}\")\n",
    "\n",
    "    args_list = []\n",
    "    for p_id, p in enumerate(cfg.ps()):\n",
    "        for lp_id, lp_alpha in enumerate(cfg.lp_alphas):\n",
    "            # Pass necessary arguments to the worker function\n",
    "            args = (\n",
    "                p_id * len(cfg.lp_alphas) + lp_id,\n",
    "                cfg.epochs,\n",
    "                p_id,\n",
    "                p,\n",
    "                lp_id,\n",
    "                lp_alpha,\n",
    "                ground_truth_features,\n",
    "                # output_queue,\n",
    "                cfg.__dict__,\n",
    "                run_num,\n",
    "                device,\n",
    "                run_num,\n",
    "            )\n",
    "            args_list.append(args)\n",
    "\n",
    "    combined_logs = []\n",
    "\n",
    "    # for arg in args_list:\n",
    "    #     ret_logs = train_model(arg)\n",
    "\n",
    "    if len(args_list) > 1:\n",
    "        with mp.Pool(\n",
    "            processes=10\n",
    "        ) as pool:  # Adjust the number of processes based on your system's capabilities\n",
    "            max_ = len(args_list)\n",
    "            with tqdm(total=max_) as pbar:\n",
    "                first_batch = True\n",
    "                for ret_logs in pool.imap_unordered(train_model, args_list):\n",
    "                    pbar.update()\n",
    "\n",
    "                    if first_batch:\n",
    "                        # For the first batch, initialize combined_logs with empty dictionaries\n",
    "                        combined_logs = [{} for _ in ret_logs]\n",
    "                        first_batch = False\n",
    "\n",
    "                    # Update each dictionary in combined_logs with the corresponding ret_log\n",
    "                    for i, log in enumerate(ret_logs):\n",
    "                        combined_logs[i].update(log)\n",
    "                        pass\n",
    "    else:\n",
    "        ret_logs = train_model(*args_list)\n",
    "        combined_logs = ret_logs\n",
    "\n",
    "    torch.save(\n",
    "        ground_truth_features,\n",
    "        f\"/root/sparsify/trained_models/toy_saes{run_num}/ground_truth_features.pt\",\n",
    "    )\n",
    "        \n",
    "    secrets = json.load(open(\"secrets.json\"))\n",
    "    wandb.login(key=secrets[\"wandb_key\"])\n",
    "    start_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    wandb_run_name = f\"Toylp_{start_time[4:]}_{cfg.batch_size}_{cfg.epochs}_{cfg.seed}\"  # trim year\n",
    "    print(f\"wandb_run_name: {wandb_run_name}\")\n",
    "    wandb.init(project=\"lp saes\", name=wandb_run_name)\n",
    "\n",
    "    for log_step in range(len(combined_logs)):\n",
    "        wandb.log(combined_logs[log_step])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lee-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
