{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bd057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "from collections.abc import Generator\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from typing import Any, List, Optional, Tuple, Union\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from fastcluster import linkage\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import ortho_group\n",
    "from torchtyping import TensorType\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311cf15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ToyArgs:\n",
    "    device: Union[str, Any] = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tied_ae: bool = False\n",
    "    seed: int = 103\n",
    "    learned_dict_ratio: float = 1.0\n",
    "    output_folder: str = \"outputs\"\n",
    "    # dtype: torch.dtype = torch.float32\n",
    "    activation_dim: int = 32\n",
    "    feature_prob_decay: float = 0.99\n",
    "    feature_num_nonzero: int = 8\n",
    "    correlated_components: bool = False\n",
    "    n_ground_truth_components: int = 128\n",
    "    batch_size: int = 4_096\n",
    "    lr: float = 4e-4\n",
    "    epochs: int = 300_000\n",
    "    n_components_dictionary: int = 256\n",
    "    n_components_dictionary_trans: int = 256\n",
    "    l1_alpha: float = 5e-3\n",
    "    use_topk: bool = False\n",
    "    topk: list[int] = field(\n",
    "        default_factory=lambda: [\n",
    "            1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16\n",
    "            # 7, 8, 9, 10, 11\n",
    "        ]\n",
    "    )\n",
    "    norm_output: bool = False\n",
    "    lp_alphas: list[float] = field(\n",
    "        # default_factory=lambda: [\n",
    "        #     1e-4,\n",
    "        #     1.8e-4,\n",
    "        #     3e-4,\n",
    "        #     5.6e-4,\n",
    "        #     1e-3,\n",
    "        #     1.8e-3,\n",
    "        #     3e-3,\n",
    "        #     5.6e-3,\n",
    "        #     1e-2,\n",
    "        #     1.8e-2,\n",
    "        #     3e-2,\n",
    "        #     5.6e-2,\n",
    "        #     1e-1,\n",
    "        #     1.8e-1,\n",
    "        #     3e-1,\n",
    "        #     5.6e-1,\n",
    "        #     1,\n",
    "        #     1.8,\n",
    "        #     3,\n",
    "        # ]\n",
    "        # default_factory=lambda: [\n",
    "        #     5.6,\n",
    "        #     1e1,\n",
    "        #     1.8e1,\n",
    "        #     3e1,\n",
    "        #     5.6e1,\n",
    "        #     1e2,\n",
    "        #     1.8e2,\n",
    "        #     3e2,\n",
    "        # ]\n",
    "        # default_factory=lambda: [ # fine\n",
    "        #     1e-2,\n",
    "        #     1.8e-2,\n",
    "        #     3e-2,\n",
    "        #     5.6e-2,\n",
    "        #     1e-1,\n",
    "        #     1.8e-1,\n",
    "        #     3e-1,\n",
    "        #     5.6e-1,\n",
    "        #     1,\n",
    "        #     1.8,\n",
    "        #     3,\n",
    "        # ]\n",
    "        default_factory=lambda: [\n",
    "            3,\n",
    "        ]\n",
    "    )\n",
    "    # lp_alphas: list[float] = field(\n",
    "    #     default_factory=lambda: [\n",
    "    #         1e-7,\n",
    "    #         3e-7,\n",
    "    #         1e-6,\n",
    "    #         3e-6,\n",
    "    #         1e-5,\n",
    "    #         3e-5,\n",
    "    #         1e-4,\n",
    "    #         3e-4,\n",
    "    #         1e-3,\n",
    "    #         3e-3,\n",
    "    #         1e-2,\n",
    "    #         3e-2,\n",
    "    #         1e-1,\n",
    "    #         3e-1,\n",
    "    #         1,\n",
    "    #         3,\n",
    "    #     ]\n",
    "    # )\n",
    "    # lp_alphas: list[float] = field(\n",
    "    #     default_factory=lambda: [\n",
    "    #         1\n",
    "    #     ]\n",
    "    # )\n",
    "    p_values: list[float] = field(\n",
    "        # default_factory=lambda: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1]\n",
    "        # default_factory=lambda: [0.1, 0.4, 0.7, 1,]\n",
    "        default_factory=lambda: [1,]\n",
    "    )\n",
    "    anneal: bool = False\n",
    "    loss_fn: str =  \"lp^p\" # options for sparsity penalty: lp_norm, lp^p, log, gated, None\n",
    "    eps: list[float] = field(\n",
    "        default_factory=lambda: [\n",
    "            0.01, 0.1\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    def ps(self):\n",
    "        if self.use_topk:\n",
    "            return self.topk\n",
    "        elif self.loss_fn==\"log\":\n",
    "            return self.eps\n",
    "        else:\n",
    "            return self.p_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc91e98",
   "metadata": {},
   "source": [
    "# Define data generators and autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea6f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RandomDatasetGenerator(Generator):\n",
    "    activation_dim: int\n",
    "    n_ground_truth_components: int\n",
    "    batch_size: int\n",
    "    feature_num_nonzero: int\n",
    "    feature_prob_decay: float\n",
    "    correlated: bool\n",
    "    device: Union[torch.device, str]\n",
    "\n",
    "    feats: Optional[TensorType[\"n_ground_truth_components\", \"activation_dim\"]] = None\n",
    "    generated_so_far: int = 0\n",
    "\n",
    "    frac_nonzero: float = field(init=False)\n",
    "    decay: TensorType[\"n_ground_truth_components\"] = field(init=False)\n",
    "    corr_matrix: Optional[\n",
    "        TensorType[\"n_ground_truth_components\", \"n_ground_truth_components\"]\n",
    "    ] = field(init=False)\n",
    "    component_probs: Optional[TensorType[\"n_ground_truth_components\"]] = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.frac_nonzero = self.feature_num_nonzero / self.n_ground_truth_components\n",
    "\n",
    "        # Define the probabilities of each component being included in the data\n",
    "        self.decay = torch.tensor(\n",
    "            [self.feature_prob_decay**i for i in range(self.n_ground_truth_components)]\n",
    "        ).to(self.device)  # FIXME: 1 / i\n",
    "\n",
    "        self.component_probs = self.decay * self.frac_nonzero  # Only if non-correlated\n",
    "        if self.feats is None:\n",
    "            self.feats = generate_rand_feats(\n",
    "                self.activation_dim,\n",
    "                self.n_ground_truth_components,\n",
    "                device=self.device,\n",
    "            )\n",
    "\n",
    "    def send(self, ignored_arg: Any) -> TensorType[\"dataset_size\", \"activation_dim\"]:\n",
    "        torch.manual_seed(self.generated_so_far)  # Set a deterministic seed for reproducibility\n",
    "        self.generated_so_far += 1\n",
    "\n",
    "        # Assuming generate_rand_dataset is your data generation function\n",
    "        _, ground_truth, data = generate_rand_dataset(\n",
    "            self.n_ground_truth_components,\n",
    "            self.batch_size,\n",
    "            self.component_probs,\n",
    "            self.feats,\n",
    "            self.device,\n",
    "        )\n",
    "        return ground_truth, data\n",
    "\n",
    "    def throw(self, type: Any = None, value: Any = None, traceback: Any = None) -> None:\n",
    "        raise StopIteration\n",
    "\n",
    "\n",
    "def generate_rand_dataset(\n",
    "    n_ground_truth_components: int,  #\n",
    "    dataset_size: int,\n",
    "    feature_probs: TensorType[\"n_ground_truth_components\"],\n",
    "    feats: TensorType[\"n_ground_truth_components\", \"activation_dim\"],\n",
    "    device: Union[torch.device, str],\n",
    ") -> Tuple[\n",
    "    TensorType[\"n_ground_truth_components\", \"activation_dim\"],\n",
    "    TensorType[\"dataset_size\", \"n_ground_truth_components\"],\n",
    "    TensorType[\"dataset_size\", \"activation_dim\"],\n",
    "]:\n",
    "    # generate random feature strengths\n",
    "    feature_strengths = torch.rand((dataset_size, n_ground_truth_components), device=device)\n",
    "    # only some features are activated, chosen at random\n",
    "    dataset_thresh = torch.rand(dataset_size, n_ground_truth_components, device=device)\n",
    "    data_zero = torch.zeros_like(dataset_thresh, device=device)\n",
    "\n",
    "    dataset_codes = torch.where(\n",
    "        dataset_thresh <= feature_probs,\n",
    "        feature_strengths,\n",
    "        data_zero,\n",
    "    )  # dim: dataset_size x n_ground_truth_components\n",
    "\n",
    "    dataset = dataset_codes @ feats\n",
    "\n",
    "    return feats, dataset_codes, dataset\n",
    "\n",
    "\n",
    "def generate_rand_feats(\n",
    "    feat_dim: int,\n",
    "    num_feats: int,\n",
    "    device: Union[torch.device, str],\n",
    ") -> TensorType[\"n_ground_truth_components\", \"activation_dim\"]:\n",
    "    data_path = os.path.join(os.getcwd(), \"data\")\n",
    "    data_filename = os.path.join(data_path, f\"feats_{feat_dim}_{num_feats}.npy\")\n",
    "\n",
    "    feats = np.random.multivariate_normal(np.zeros(feat_dim), np.eye(feat_dim), size=num_feats)\n",
    "    feats = feats.T / np.linalg.norm(feats, axis=1)\n",
    "    feats = feats.T\n",
    "\n",
    "    feats_tensor = torch.from_numpy(feats).to(device).float()\n",
    "    return feats_tensor\n",
    "\n",
    "\n",
    "# AutoEncoder Definition\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, activation_size, n_dict_components):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(nn.Linear(activation_size, n_dict_components), nn.ReLU())\n",
    "        self.decoder = nn.Linear(n_dict_components, activation_size, bias=False)\n",
    "\n",
    "        # Initialize the decoder weights orthogonally\n",
    "        nn.init.orthogonal_(self.decoder.weight)\n",
    "\n",
    "    def forward(self, x, topk=None, norm_output=False):\n",
    "        c = self.encoder(x)\n",
    "        if topk is not None:\n",
    "            _, indices = torch.topk(c, topk, dim=-1, largest=True, sorted=False)\n",
    "            zero_tensor = torch.zeros_like(c)\n",
    "            zero_tensor.scatter_(-1, indices, c.gather(-1, indices))\n",
    "            c = zero_tensor\n",
    "\n",
    "        # Apply unit norm constraint to the decoder weights\n",
    "        self.decoder.weight.data = nn.functional.normalize(self.decoder.weight.data, dim=0)\n",
    "\n",
    "        x_hat = self.decoder(c)\n",
    "        \n",
    "        if norm_output:            \n",
    "            x_hat_norm = torch.norm(x_hat, p=2, dim=1, keepdim=True)\n",
    "            # If the norm is 0, we can't divide by it\n",
    "            x_hat = x_hat / torch.clamp(x_hat_norm, min=1e-8)\n",
    "            c = c / torch.clamp(x_hat_norm, min=1e-8)\n",
    "            \n",
    "        return x_hat, c\n",
    "    \n",
    "    def get_dictionary(self):\n",
    "        self.decoder.weight.data = nn.functional.normalize(self.decoder.weight.data, dim=0)\n",
    "        return self.decoder.weight\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "\n",
    "def cosine_sim(\n",
    "    vecs1: Union[torch.Tensor, torch.nn.parameter.Parameter, npt.NDArray],\n",
    "    vecs2: Union[torch.Tensor, torch.nn.parameter.Parameter, npt.NDArray],\n",
    ") -> np.ndarray:\n",
    "    vecs = [vecs1, vecs2]\n",
    "    for i in range(len(vecs)):\n",
    "        if not isinstance(vecs[i], np.ndarray):\n",
    "            vecs[i] = vecs[i].detach().cpu().numpy()  # type: ignore\n",
    "    vecs1, vecs2 = vecs\n",
    "    normalize = lambda v: (v.T / np.linalg.norm(v, axis=1)).T\n",
    "    vecs1_norm = normalize(vecs1)\n",
    "    vecs2_norm = normalize(vecs2)\n",
    "\n",
    "    return vecs1_norm @ vecs2_norm.T\n",
    "\n",
    "\n",
    "def mean_max_cosine_similarity(ground_truth_features, learned_dictionary, debug=False):\n",
    "    # Calculate cosine similarity between all pairs of ground truth and learned features\n",
    "    cos_sim = cosine_sim(ground_truth_features, learned_dictionary)\n",
    "    # Find the maximum cosine similarity for each ground truth feature, then average\n",
    "    mmcs = cos_sim.max(axis=1).mean()\n",
    "    return mmcs\n",
    "\n",
    "\n",
    "def calculate_mmcs(auto_encoder, ground_truth_features):\n",
    "    learned_dictionary = auto_encoder.decoder.weight.data.t()\n",
    "    with torch.no_grad():\n",
    "        mmcs = mean_max_cosine_similarity(\n",
    "            ground_truth_features.to(auto_encoder.device), learned_dictionary\n",
    "        )\n",
    "    return mmcs\n",
    "\n",
    "\n",
    "def get_alive_neurons(auto_encoder, data_generator, n_batches=10, topk=None):\n",
    "    \"\"\"\n",
    "    :param result_dict: dictionary containing the results of a single run\n",
    "    :return: number of dead neurons\n",
    "\n",
    "    Estimates the number of dead neurons in the network by running a few batches of data through the network and\n",
    "    calculating the mean activation of each neuron. If the mean activation is 0 for a neuron, it is considered dead.\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    for i in range(n_batches):\n",
    "        ground_truth, batch = next(data_generator)\n",
    "        x_hat, c = auto_encoder(\n",
    "            batch, topk=topk\n",
    "        )  # x_hat: (batch_size, activation_dim), c: (batch_size, n_dict_components)\n",
    "        outputs.append(c)\n",
    "    outputs = torch.cat(outputs)  # (n_batches * batch_size, n_dict_components)\n",
    "    mean_activations = outputs.mean(\n",
    "        dim=0\n",
    "    )  # (n_dict_components), c is after the ReLU, no need to take abs\n",
    "    alive_neurons = mean_activations > 0\n",
    "    return alive_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = ToyArgs()\n",
    "cfg.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f6b40b",
   "metadata": {},
   "source": [
    "## Loading Saved SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdced1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saes\n",
    "seed_string = f\"{cfg.seed}\" if cfg.seed >= 0 else \"\"\n",
    "print(f\"loading from toy_saes{seed_string}\")\n",
    "auto_encoders = [[None for l1_coef in cfg.lp_alphas] for p in cfg.ps()]\n",
    "for p_id, p in enumerate(cfg.ps()):\n",
    "    for lp_id, lp_alpha in enumerate(cfg.lp_alphas):\n",
    "        save_name = f\"sae_l{p}_{lp_alpha}\"\n",
    "        auto_encoder = torch.load(\n",
    "            f\"/root/sparsify/trained_models/toy_saes{seed_string}/{save_name}.pt\"\n",
    "        )\n",
    "        auto_encoders[p_id][lp_id] = auto_encoder\n",
    "\n",
    "# load ground truth features\n",
    "ground_truth_features = torch.load(\n",
    "    f\"/root/sparsify/trained_models/toy_saes{seed_string}/ground_truth_features.pt\"\n",
    ")\n",
    "data_generator = RandomDatasetGenerator(\n",
    "    activation_dim=cfg.activation_dim,\n",
    "    n_ground_truth_components=cfg.n_ground_truth_components,\n",
    "    batch_size=cfg.batch_size,\n",
    "    feature_num_nonzero=cfg.feature_num_nonzero,\n",
    "    feature_prob_decay=cfg.feature_prob_decay,\n",
    "    correlated=cfg.correlated_components,\n",
    "    device=device,\n",
    "    generated_so_far=cfg.seed,\n",
    "    feats=ground_truth_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78729fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator.component_probs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee7d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_topk = torch.zeros((len(cfg.ps()), len(cfg.lp_alphas)))\n",
    "if cfg.use_topk and False:\n",
    "    # recompute losses\n",
    "    additional_topks = range(24)\n",
    "    final_reconstruction = torch.zeros((len(cfg.ps()), len(cfg.lp_alphas), len(additional_topks)))\n",
    "    final_lp = torch.zeros((len(cfg.ps()), len(cfg.lp_alphas), len(additional_topks)))\n",
    "\n",
    "    num_inner_epochs = 100\n",
    "    for epoch in tqdm(range(num_inner_epochs)):\n",
    "        ground_truth, batch = next(data_generator)\n",
    "\n",
    "        for p_id, p in enumerate(cfg.ps()):\n",
    "            for lp_id, lp_alpha in enumerate(cfg.lp_alphas):\n",
    "                with torch.no_grad():\n",
    "                    auto_encoder = auto_encoders[p_id][lp_id]\n",
    "                    for t_id, add_topk in enumerate(additional_topks):\n",
    "\n",
    "                        # Forward pass\n",
    "                        topk = p if cfg.use_topk else None\n",
    "                        x_hat, c = auto_encoder(batch, topk=topk + add_topk, norm_output=cfg.norm_output)\n",
    "                        # also normalize output for accuracte reconstruction loss:\n",
    "                        if cfg.norm_output:\n",
    "                            x_hat = x_hat * torch.norm(batch, p=2, dim=1, keepdim=True)\n",
    "                            c = c * torch.norm(batch, p=2, dim=1, keepdim=True)\n",
    "                            # batch = batch/torch.clamp(torch.norm(batch, p=2, dim=1, keepdim=True), min=1e-8)\n",
    "\n",
    "                        # Compute the reconstruction loss and L1 regularization\n",
    "                        l_reconstruction = torch.nn.MSELoss()(batch, x_hat)\n",
    "                        l_lp = lp_alpha * torch.norm(c, p, dim=1).mean() / c.size(1)\n",
    "\n",
    "                        final_reconstruction[p_id, lp_id, t_id] += l_reconstruction.detach().cpu().clone()\n",
    "                        final_lp[p_id, lp_id, t_id] += l_lp.detach().cpu().clone()\n",
    "\n",
    "    final_reconstruction = final_reconstruction/num_inner_epochs\n",
    "    final_lp = final_lp/num_inner_epochs\n",
    "\n",
    "    # calculate best add_topk:\n",
    "    opt_topk = torch.argmax(-final_reconstruction, dim=-1)\n",
    "    final_reconstruction = final_reconstruction[torch.arange(final_reconstruction.size(0)).unsqueeze(1), torch.arange(final_reconstruction.size(1)), opt_topk]\n",
    "    final_lp = final_lp[torch.arange(final_lp.size(0)).unsqueeze(1), torch.arange(final_lp.size(1)), opt_topk]\n",
    "    \n",
    "else:\n",
    "    # recompute losses\n",
    "    final_reconstruction = torch.zeros((len(cfg.ps()), len(cfg.lp_alphas)))\n",
    "    final_lp = torch.zeros((len(cfg.ps()), len(cfg.lp_alphas)))\n",
    "\n",
    "    num_inner_epochs = 100\n",
    "    for epoch in tqdm(range(num_inner_epochs)):\n",
    "        ground_truth, batch = next(data_generator)\n",
    "\n",
    "        for p_id, p in enumerate(cfg.ps()):\n",
    "            for lp_id, lp_alpha in enumerate(cfg.lp_alphas):\n",
    "                with torch.no_grad():\n",
    "                    auto_encoder = auto_encoders[p_id][lp_id]\n",
    "\n",
    "                    # Forward pass\n",
    "                    topk = p if cfg.use_topk else None                    \n",
    "                    x_hat, c = auto_encoder(batch, topk=topk, norm_output=cfg.norm_output)\n",
    "                    # print(\"xhat norm\", torch.norm(x_hat, p=2, dim=1, keepdim=True))\n",
    "                    # also normalize output for accuracte reconstruction loss:\n",
    "                    if cfg.norm_output:\n",
    "                        x_hat = x_hat * torch.norm(batch, p=2, dim=1, keepdim=True)\n",
    "                        c = c * torch.norm(batch, p=2, dim=1, keepdim=True)\n",
    "                        # batch = batch/torch.clamp(torch.norm(batch, p=2, dim=1, keepdim=True), min=1e-8)\n",
    "\n",
    "                    # Compute the reconstruction loss and L1 regularization\n",
    "                    l_reconstruction = torch.nn.MSELoss()(batch, x_hat)\n",
    "                    \n",
    "                    if cfg.loss_fn == \"lp_norm\":\n",
    "                        l_lp = lp_alpha * torch.norm(c, p, dim=1).mean() / c.size(1)\n",
    "                    elif cfg.loss_fn == \"lp^p\":\n",
    "                        l_lp = lp_alpha *torch.pow(c, p).sum(dim=-1).mean() / c.size(1)\n",
    "\n",
    "                    final_reconstruction[p_id, lp_id] += l_reconstruction.detach().cpu().clone()\n",
    "                    final_lp[p_id, lp_id] += l_lp.detach().cpu().clone()\n",
    "\n",
    "    final_reconstruction = final_reconstruction/num_inner_epochs\n",
    "    final_lp = final_lp/num_inner_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b8db8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def effective_rank(matrix: torch.Tensor) -> torch.Tensor:\n",
    "    # For each row of a matrix, return the effective count of the non-zero values.\n",
    "    # If all the nonzero values are the same, this equals the number of nonzero values\n",
    "\n",
    "    matrix = torch.clamp(matrix, min=0.0)  # should be unnecessary\n",
    "    normalized_matrix = (matrix / matrix.sum(dim=-1, keepdim=True)).nan_to_num(0)\n",
    "    plogp = (normalized_matrix * normalized_matrix.log()).nan_to_num(0)\n",
    "    shannon_entropy = -plogp.sum(dim=-1)\n",
    "    return shannon_entropy.exp()\n",
    "\n",
    "\n",
    "def centroid(matrix: torch.Tensor) -> torch.Tensor:\n",
    "    # For each row of a matrix, return the effective count of the non-zero values.\n",
    "    # If all the nonzero values are the same, this equals the number of nonzero values\n",
    "\n",
    "    matrix = torch.clamp(matrix, min=0.0)  # should be unnecessary\n",
    "    matrix, indices = torch.sort(matrix, dim=1, descending=True)\n",
    "    # normalized_matrix = (matrix / matrix.sum(dim=-1, keepdim=True)).nan_to_num(0)\n",
    "    positions = torch.arange(matrix.shape[1], device=matrix.device).unsqueeze(0)\n",
    "    centroid = (matrix * positions).sum(dim=-1, keepdim=True) / matrix.sum(dim=-1, keepdim=True)\n",
    "    return centroid.nan_to_num(0) + 1\n",
    "\n",
    "\n",
    "def count_top(matrix: torch.Tensor, threshold=0.5) -> torch.Tensor:\n",
    "    # For each row of a matrix, return the number of values greater than threshold times max\n",
    "\n",
    "    matrix = torch.clamp(matrix, min=0.0)  # should be unnecessary\n",
    "    max_sim = matrix.max(dim=-1, keepdim=True)[0]\n",
    "\n",
    "    return (matrix >= max_sim * threshold).sum(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "def effective_count(matrix: torch.Tensor) -> torch.Tensor:\n",
    "    return count_top(matrix)\n",
    "\n",
    "\n",
    "def similarity_concentration_oneway(matrix, filter_dead_rows=False):\n",
    "    # half of the full metric, using only one direction\n",
    "    # (either just monosemanticity or superposition checking)\n",
    "    if matrix.shape[-1] == 0:\n",
    "        return 0\n",
    "    max_sim = matrix.max(dim=-1, keepdim=True)[0]\n",
    "    eff_feature_count = effective_count(matrix)\n",
    "    rowwise_concentration = max_sim / eff_feature_count\n",
    "    if filter_dead_rows:\n",
    "        rowwise_concentration = rowwise_concentration[max_sim > 0]\n",
    "    return rowwise_concentration.mean()\n",
    "\n",
    "\n",
    "def similarity_concentration(matrix, ground=None, empirical=None, alive_neurons=None):\n",
    "    \"\"\"\n",
    "    Compute the similarity_concentration metric given:\n",
    "        matrix[ground_feature, sae_feature]: number of co-occurrences, or cosine similarities\n",
    "        ground[ground_feature]: total number of appearances of ground features, or None if cosine_sims\n",
    "        empirical[sae_feature]: total number of appearances of empirical features, or None if cosine_sims\n",
    "    method: (max(M)/effective_count(M)) for both matrix and matrix.mT, then combined\n",
    "    \"\"\"\n",
    "\n",
    "    if ground is None:\n",
    "        ground = torch.ones_like(matrix[:, 0])\n",
    "    if empirical is None:\n",
    "        empirical = torch.ones_like(matrix[0, :])\n",
    "\n",
    "    if alive_neurons is None:\n",
    "        alive_neurons = torch.full_like(matrix[0], True)\n",
    "\n",
    "    matrix = matrix[:, alive_neurons]\n",
    "    empirical = empirical[alive_neurons]\n",
    "\n",
    "    ground_truth_sc = similarity_concentration_oneway(matrix / ground[:, None])\n",
    "    sae_feature_sc = similarity_concentration_oneway(\n",
    "        matrix.mT / empirical[:, None], filter_dead_rows=True\n",
    "    )\n",
    "\n",
    "    return (ground_truth_sc * sae_feature_sc).sqrt(), ground_truth_sc, sae_feature_sc  # geo mean\n",
    "\n",
    "# def zero_out_except_topk(tensor, topk):\n",
    "#     # Keep only the topk values, set others to zero\n",
    "#     _, indices = torch.topk(tensor, topk, dim=-1, largest=True, sorted=False)\n",
    "#     zero_tensor = torch.zeros_like(tensor)\n",
    "#     zero_tensor.scatter_(-1, indices, tensor.gather(-1, indices))\n",
    "#     return zero_tensor\n",
    "\n",
    "class BatchCorrelationCalculator:\n",
    "    def __init__(self, feature_dim_x, feature_dim_y, device=\"cuda\"):\n",
    "        # Initialize sums needed for correlation calculation for each feature dimension\n",
    "        self.sum_x = torch.zeros(feature_dim_x, device=device)\n",
    "        self.sum_y = torch.zeros(feature_dim_y, device=device)\n",
    "        self.sum_x2 = torch.zeros(feature_dim_x, device=device)\n",
    "        self.sum_y2 = torch.zeros(feature_dim_y, device=device)\n",
    "        self.sum_xy = torch.zeros(\n",
    "            (feature_dim_x, feature_dim_y), device=device\n",
    "        )  # This now becomes a matrix\n",
    "        self.n = 0\n",
    "        self.device = device\n",
    "\n",
    "    def update(self, x_batch, y_batch):\n",
    "        # Update running sums with a new batch\n",
    "        self.sum_x += torch.sum(x_batch, dim=0)\n",
    "        self.sum_y += torch.sum(y_batch, dim=0)\n",
    "        self.sum_x2 += torch.sum(x_batch**2, dim=0)\n",
    "        self.sum_y2 += torch.sum(y_batch**2, dim=0)\n",
    "        self.sum_xy += torch.einsum(\"bg, bs -> gs\", x_batch, y_batch)\n",
    "        self.n += x_batch.shape[0]\n",
    "\n",
    "    def compute_correlation(self):\n",
    "        # Compute Pearson correlation coefficient matrix between features of the two vectors\n",
    "        numerator = self.n * self.sum_xy - torch.ger(self.sum_x, self.sum_y)\n",
    "        denominator = torch.sqrt(\n",
    "            torch.ger(self.n * self.sum_x2 - self.sum_x**2, self.n * self.sum_y2 - self.sum_y**2)\n",
    "        )\n",
    "\n",
    "        # Handle division by zero for cases with no variance\n",
    "        valid = denominator != 0\n",
    "        correlation_matrix = torch.zeros_like(denominator)\n",
    "        correlation_matrix[valid] = numerator[valid] / denominator[valid]\n",
    "\n",
    "        # Set correlations to 0 where denominator is 0 (indicating no variance)\n",
    "        correlation_matrix[~valid] = 0\n",
    "\n",
    "        return correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb02ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute similarity concentration for each sae\n",
    "sim_conc_co_occur = torch.zeros(\n",
    "    (len(cfg.ps()), len(cfg.lp_alphas), 3)\n",
    ")  # p, lp_alpha, ground or sae sc\n",
    "sim_conc_cosim = torch.zeros((len(cfg.ps()), len(cfg.lp_alphas), 3))  # p, lp_alpha, ground or sae sc\n",
    "sim_conc_corr = torch.zeros((len(cfg.ps()), len(cfg.lp_alphas), 3))  # p, lp_alpha, ground or sae sc\n",
    "mmcs = torch.zeros((len(cfg.ps()), len(cfg.lp_alphas)))\n",
    "features_per_ground = torch.zeros((len(cfg.ps()), len(cfg.lp_alphas)))\n",
    "l2_ratio = torch.zeros((len(cfg.ps()), len(cfg.lp_alphas)))\n",
    "l0 = torch.zeros((len(cfg.ps()), len(cfg.lp_alphas)))\n",
    "\n",
    "for p_id, p in enumerate(tqdm(cfg.ps())):\n",
    "    for lp_id, lp_alpha in enumerate(cfg.lp_alphas):\n",
    "        auto_encoder = auto_encoders[p_id][lp_id]\n",
    "\n",
    "        # adjacency_matrix[ground_truth_feature,sae_feature] = number of times they co-occur\n",
    "        adjacency_matrix = torch.zeros(\n",
    "            (cfg.n_ground_truth_components, cfg.n_components_dictionary), device=device\n",
    "        )\n",
    "\n",
    "        # iteratively calculate correlations\n",
    "        corr_calculator = BatchCorrelationCalculator(\n",
    "            cfg.n_ground_truth_components, cfg.n_components_dictionary, device=cfg.device\n",
    "        )\n",
    "\n",
    "        # feat_corr_calculator = BatchCorrelationCalculator(\n",
    "        #     cfg.n_components_dictionary, cfg.n_components_dictionary, device=cfg.device\n",
    "        # )\n",
    "\n",
    "        cosim_matrix = data_generator.feats @ nn.functional.normalize(\n",
    "            auto_encoder.decoder.weight.data, dim=0\n",
    "        )  # of shape (ground_feats, sae_feats)\n",
    "        mmcs[p_id, lp_id] = cosim_matrix.max(dim=-1)[0].mean()\n",
    "        # might need to flip data_generator.feats?\n",
    "        ground_truth_appearances = torch.zeros((cfg.n_ground_truth_components), device=device)\n",
    "        sae_appearances = torch.zeros((cfg.n_components_dictionary), device=device)\n",
    "\n",
    "        inp_norm = 0\n",
    "        outp_norm = 0\n",
    "\n",
    "        # Filter out dead neurons: either do it out here, or inside the metric\n",
    "        topk = int(p + opt_topk[p_id,lp_id]) if cfg.use_topk else None\n",
    "        alive = get_alive_neurons(auto_encoder, data_generator, topk=topk)\n",
    "\n",
    "        num_inner_epochs = 4\n",
    "        for epoch in range(num_inner_epochs):  # range(cfg.epochs):\n",
    "            ground_truth, batch = next(data_generator)\n",
    "\n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                x_hat1, c1 = auto_encoder(batch, topk=topk, norm_output=cfg.norm_output)\n",
    "                \n",
    "                # also normalize output for accuracte reconstruction loss:\n",
    "                if cfg.norm_output:\n",
    "                    x_hat1 = x_hat1 * torch.norm(batch, p=2, dim=1, keepdim=True)\n",
    "                    c1 = c1 * torch.norm(batch, p=2, dim=1, keepdim=True)\n",
    "                    # batch = batch/torch.clamp(torch.norm(batch, p=2, dim=1, keepdim=True), min=1e-8)\n",
    "                \n",
    "                # x_hat2, c2 = auto_encoder2(Mbatch)\n",
    "                inp_norm += batch.norm(p=2, dim=-1).mean()\n",
    "                outp_norm += x_hat1.norm(p=2, dim=-1).mean()\n",
    "                \n",
    "                corr_calculator.update(ground_truth, c1) # calc corr before binarizing\n",
    "                # feat_corr_calculator.update(c1, c1)\n",
    "\n",
    "                ground_truth[ground_truth != 0] = 1\n",
    "                c1[c1 != 0] = 1\n",
    "\n",
    "                adjacency_matrix += torch.einsum(\"bg, bs -> gs\", ground_truth, c1)\n",
    "                \n",
    "                ground_truth_appearances += ground_truth.sum(dim=0)\n",
    "                sae_appearances += c1.sum(dim=0)\n",
    "                l0[p_id, lp_id] += c1.sum(dim=-1).mean().cpu()\n",
    "\n",
    "        l0[p_id, lp_id] = l0[p_id, lp_id] / num_inner_epochs\n",
    "\n",
    "        sc, gsc, ssc = similarity_concentration(\n",
    "            adjacency_matrix, ground_truth_appearances, sae_appearances, alive_neurons=alive\n",
    "        )\n",
    "        sim_conc_co_occur[p_id, lp_id, :] = torch.tensor([sc, gsc, ssc])\n",
    "\n",
    "        sc, gsc, ssc = similarity_concentration(cosim_matrix, alive_neurons=alive)\n",
    "        sim_conc_cosim[p_id, lp_id, :] = torch.tensor([sc, gsc, ssc])\n",
    "\n",
    "        corr_matrix = corr_calculator.compute_correlation()\n",
    "        sc, gsc, ssc = similarity_concentration(corr_matrix, alive_neurons=alive)\n",
    "        sim_conc_corr[p_id, lp_id, :] = torch.tensor([sc, gsc, ssc])\n",
    "\n",
    "        features_per_ground[p_id, lp_id] = (adjacency_matrix > 0).sum(dim=-1).mean(dtype=float)\n",
    "        l2_ratio[p_id, lp_id] = outp_norm / inp_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9b58f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = f\"metrics_{cfg.seed}\"\n",
    "if not os.path.exists(f\"/root/sparsify/notebooks/{pre}\"):\n",
    "    os.makedirs(f\"/root/sparsify/notebooks/{pre}\")\n",
    "torch.save(sim_conc_co_occur, f\"{pre}/sim_conc_co_occur_{cfg.seed}.pt\")\n",
    "torch.save(sim_conc_corr, f\"{pre}/sim_conc_corr_{cfg.seed}.pt\")\n",
    "torch.save(sim_conc_cosim, f\"{pre}/sim_conc_cosim_{cfg.seed}.pt\")\n",
    "torch.save(mmcs, f\"{pre}/mmcs_{cfg.seed}.pt\")\n",
    "torch.save(features_per_ground, f\"{pre}/features_per_ground_{cfg.seed}.pt\")\n",
    "torch.save(l2_ratio, f\"{pre}/l2_ratio_{cfg.seed}.pt\")\n",
    "torch.save(l0, f\"{pre}/l0_{cfg.seed}.pt\")\n",
    "torch.save(final_reconstruction, f\"{pre}/final_reconstruction_{cfg.seed}.pt\")\n",
    "torch.save(final_lp, f\"{pre}/final_lp_{cfg.seed}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0949855",
   "metadata": {},
   "source": [
    "# Load computed matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a023f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mean(seeds, matrix_string):    \n",
    "    matrices = []\n",
    "    for seed in seeds:\n",
    "        pre = f\"metrics_{seed}\"\n",
    "        path = f\"{pre}/{matrix_string}_{seed}.pt\"\n",
    "        matrices.append(torch.load(path))\n",
    "    return sum(matrices)/len(matrices)\n",
    "\n",
    "seeds = [cfg.seed,]\n",
    "# seeds = [15,16,17,18]  # lp_norm\n",
    "# seeds = [22,24,25]    # lp^p\n",
    "# seeds = [22]\n",
    "sim_conc_co_occur = load_mean(seeds, \"sim_conc_co_occur\")\n",
    "sim_conc_corr = load_mean(seeds, \"sim_conc_corr\")\n",
    "sim_conc_cosim = load_mean(seeds, \"sim_conc_cosim\")\n",
    "mmcs = load_mean(seeds, \"mmcs\")\n",
    "features_per_ground = load_mean(seeds, \"features_per_ground\")\n",
    "l2_ratio = load_mean(seeds, \"l2_ratio\")\n",
    "l0 = load_mean(seeds, \"l0\")\n",
    "final_reconstruction = load_mean(seeds, \"final_reconstruction\")\n",
    "final_lp = load_mean(seeds, \"final_lp\")\n",
    "\n",
    "if not os.path.exists(f\"/root/sparsify/notebooks/images_{cfg.seed}\"):\n",
    "    os.makedirs(f\"/root/sparsify/notebooks/images_{cfg.seed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b3f896",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.loss_fn==\"lp_norm\":\n",
    "    l0 = torch.cat([load_mean(seeds, \"l0\") for seeds in [[15,16,17,18],[21]]], dim=1)\n",
    "    final_reconstruction = torch.cat([load_mean(seeds, \"final_reconstruction\") for seeds in [[15,16,17,18],[21]]], dim=1)\n",
    "    \n",
    "if cfg.loss_fn==\"lp^p\":\n",
    "    l0 = torch.cat([load_mean(seeds, \"l0\") for seeds in [[22,24,25],[26]]], dim=1)\n",
    "    final_reconstruction = torch.cat([load_mean(seeds, \"final_reconstruction\") for seeds in [[22,24,25],[26]]], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e63c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(l2_ratio)  # cosim_matrix.detach().cpu()\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Lp Coefficient\")\n",
    "plt.ylabel(\"p norm\")\n",
    "plt.title(\"L2 Ratio\")\n",
    "\n",
    "plt.xticks(range(len(cfg.lp_alphas)), cfg.lp_alphas, rotation=30)\n",
    "plt.yticks(\n",
    "    range(len(cfg.ps())),\n",
    "    cfg.ps(),\n",
    ")\n",
    "plt.savefig(f\"images_{cfg.seed}/l2_ratio.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87b174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c019570",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c00d1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea0ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(final_reconstruction.log10())  # cosim_matrix.detach().cpu()\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Lp Coefficient\")\n",
    "plt.ylabel(\"p norm\")\n",
    "plt.title(\"Reconstruction Loss\")\n",
    "\n",
    "plt.xticks(range(len(cfg.lp_alphas)), cfg.lp_alphas, rotation=30)\n",
    "plt.yticks(\n",
    "    range(len(cfg.ps())),\n",
    "    cfg.ps(),\n",
    ")\n",
    "plt.savefig(f\"images_{cfg.seed}/reconstruction.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0219b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_reversed = plt.cm.get_cmap('viridis_r')\n",
    "plt.imshow(l0, cmap=cmap_reversed)  # cosim_matrix.detach().cpu()\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Lp Coefficient\")\n",
    "plt.ylabel(\"p norm\")\n",
    "plt.title(\"L0\")\n",
    "\n",
    "plt.xticks(range(len(cfg.lp_alphas)), cfg.lp_alphas, rotation=30)\n",
    "plt.yticks(\n",
    "    range(len(cfg.ps())),\n",
    "    cfg.ps(),\n",
    ")\n",
    "plt.savefig(f\"images_{cfg.seed}/L0.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac09aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pareto curve\n",
    "for p in cfg.ps():\n",
    "    p_id = cfg.ps().index(p)\n",
    "    print(p_id)\n",
    "    l0_p = l0[p_id]\n",
    "    mse_p = final_reconstruction[p_id]\n",
    "    # l0 = (df_filtered[\"l0\"] + df_filtered[\"l02\"])/2\n",
    "    # mse = (df_filtered[\"mse\"] + df_filtered[\"mse2\"])/2\n",
    "    plt.plot(l0_p, np.log10(mse_p), label=f\"L{p}\")\n",
    "\n",
    "plt.axvline(6, linestyle=\"dashed\", c=\"green\")\n",
    "plt.axvline(32, linestyle=\"dashed\", c=\"black\")\n",
    "plt.xlabel(\"L0\")\n",
    "plt.ylabel(\"Log10(MSE)\")\n",
    "plt.legend()\n",
    "plt.title(\"Toy SAEs Pareto Curve\")\n",
    "plt.savefig(f\"images_{cfg.seed}/pareto.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d3b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# palette = sns.color_palette()\n",
    "# sns.color_palette()\n",
    "palette = sns.color_palette(\"magma_r\")\n",
    "display(palette)\n",
    "# palette = [palette[i] for i in [0,1,3,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6332b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pareto curve\n",
    "plt.style.use('default')\n",
    "plt.axvline(8, linestyle=\"dashed\", c=\"blue\")\n",
    "plt.axvline(32, linestyle=\"dashed\", c=\"black\")\n",
    "\n",
    "method_string = cfg.loss_fn\n",
    "if cfg.loss_fn==\"lp_norm\":\n",
    "    method_string = \"$L_p$\"\n",
    "    use_ps = cfg.ps()[3::2]\n",
    "    palette_generator = iter([palette[i] for i in [1,2,3,5]])\n",
    "if cfg.loss_fn==\"lp^p\":\n",
    "    method_string = \"$L_p^p$\"\n",
    "    use_ps = cfg.ps()[1::2]\n",
    "    palette_generator = iter([palette[i] for i in [0,1,2,3,5]])\n",
    "\n",
    "\n",
    "for p in use_ps:\n",
    "    p_id = cfg.ps().index(p)\n",
    "    print(p_id)\n",
    "    color = next(palette_generator)\n",
    "    l0_p = l0[p_id]\n",
    "    mse_p = final_reconstruction[p_id]\n",
    "    # l0 = (df_filtered[\"l0\"] + df_filtered[\"l02\"])/2\n",
    "    # mse = (df_filtered[\"mse\"] + df_filtered[\"mse2\"])/2\n",
    "    \n",
    "    # label = f\"$L_{{{p}}}$\" if cfg.loss_fn==\"lp_norm\" else f\"$L_{{{p}}}^{{{p}}}$\"\n",
    "    label = f\"p={p}\"\n",
    "    plt.plot(l0_p, np.log10(mse_p), label=label, color=color)\n",
    "    \n",
    "    # add scatter plot for individual points\n",
    "    seeds = [15,16,17,18,21] if cfg.loss_fn==\"lp_norm\" else [22,24,25]\n",
    "    for seed in seeds:\n",
    "        l0_seed = load_mean([seed], \"l0\")[p_id]\n",
    "        mse_seed = load_mean([seed], \"final_reconstruction\")[p_id]\n",
    "        plt.scatter(l0_seed, np.log10(mse_seed), s=10, color=color)\n",
    "\n",
    "\n",
    "plt.xlabel(\"$L_0$\")\n",
    "plt.ylabel(\"$\\log_{10}(MSE)$\")\n",
    "plt.xlim(left=-2,right=35)\n",
    "plt.legend()\n",
    "plt.title(f\"Synthetic Data MSE vs. $L_0$, with penalty {method_string}\")\n",
    "plt.savefig(f\"images_{cfg.seed}/pareto.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717ed9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(features_per_ground)  # cosim_matrix.detach().cpu()\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Lp Coefficient\")\n",
    "plt.ylabel(\"p norm\")\n",
    "plt.title(\"Number of Active Features\")\n",
    "\n",
    "plt.xticks(range(len(cfg.lp_alphas)), cfg.lp_alphas, rotation=30)\n",
    "plt.yticks(\n",
    "    range(len(cfg.ps())),\n",
    "    cfg.ps(),\n",
    ")\n",
    "plt.savefig(f\"images_{cfg.seed}/active.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10a10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_id = 7\n",
    "l1_id = 10\n",
    "cosim_matrix = (\n",
    "    (\n",
    "        data_generator.feats\n",
    "        @ nn.functional.normalize(auto_encoders[p_id][l1_id].decoder.weight.data, dim=0)\n",
    "    )\n",
    "    .detach()\n",
    "    .cpu()\n",
    ")\n",
    "sorted_cosim_matrix = cosim_matrix.sort(descending=True, dim=-1)[0].cpu()\n",
    "plt.imshow(sorted_cosim_matrix)  # cosim_matrix.detach().cpu()\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Sorted SAE features\")\n",
    "plt.ylabel(\"Ground truth features\")\n",
    "plt.title(f\"Sorted Cosim matrix for p {cfg.ps()[p_id]}, lp_alpha {cfg.lp_alphas[l1_id]}\")\n",
    "\n",
    "# plt.xticks(range(len(cfg.lp_alphas)), cfg.lp_alphas, rotation=30)\n",
    "# plt.yticks(\n",
    "#     range(len(cfg.ps)),\n",
    "#     cfg.ps,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d1577",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_cosim = cosim_matrix[0].clamp(min=0).sort(descending=True)[0].cpu()\n",
    "threshold = 0.5\n",
    "plt.plot(sorted_cosim)\n",
    "plt.axvline(effective_rank(cosim_matrix[0:1]).cpu().item(), label=\"Effective Rank\", color=\"black\")\n",
    "plt.axvline(centroid(cosim_matrix[0:1]).cpu().item(), label=\"Center of Mass\", color=\"brown\")\n",
    "plt.axvline(\n",
    "    count_top(cosim_matrix[0:1], threshold=0.5).cpu().item(),\n",
    "    label=f\">{threshold}*max\",\n",
    "    color=\"green\",\n",
    ")\n",
    "plt.title(\"Different Effective Count Methods\")\n",
    "plt.xlabel(\"Sorted Features\")\n",
    "plt.ylabel(\"Cosine Similarity to True Feature 0\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fc9971",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mmcs)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Lp Coefficient\")\n",
    "plt.ylabel(\"p norm\")\n",
    "plt.title(\"MMCS\")\n",
    "\n",
    "plt.xticks(range(len(cfg.lp_alphas)), cfg.lp_alphas, rotation=30)\n",
    "plt.yticks(\n",
    "    range(len(cfg.ps())),\n",
    "    cfg.ps(),\n",
    ")\n",
    "plt.savefig(f\"images_{cfg.seed}/mmcs.png\")\n",
    "mmcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7241e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use either sim_conc_cosim or sim_conc_co_occur\n",
    "# plt.imshow(sim_conc_cosim[...,0]) #.mean(dim=-1))\n",
    "plt.imshow(mmcs)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Lp Coefficient\")\n",
    "plt.ylabel(\"p norm\")\n",
    "plt.title(\"MMCS\")\n",
    "\n",
    "plt.xticks(range(len(cfg.lp_alphas)), cfg.lp_alphas, rotation=30)\n",
    "plt.yticks(\n",
    "    range(len(cfg.ps())),\n",
    "    cfg.ps(),\n",
    ")\n",
    "plt.savefig(f\"images_{cfg.seed}/mmcs.png\")\n",
    "\n",
    "# for every ground truth feature, you take the maximum cosine similarity with any of the learned SAE features\n",
    "# take the mean over all ground truth features\n",
    "\n",
    "# Lp = |x - x'|_p  \"almost a norm\"\n",
    "#    =  [ Sum_i (x_i - x'_i)^p ]^1/p\n",
    "\n",
    "# Lp^p = |x - x'|_p^p  - has diminishing returns to scale, treats each feature independently\n",
    "#      = Sum_i (x_i - x'_i)^p\n",
    "\n",
    "# Lp^p -> L0\n",
    "# Lp does not limit to L0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7506b0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmcs.max()  # no-anneal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b88c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b846e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use either sim_conc_cosim or sim_conc_co_occur\n",
    "# plt.imshow(sim_conc_cosim[...,0]) #.mean(dim=-1))\n",
    "plt.imshow(sim_conc_cosim[..., 0])\n",
    "plt.colorbar()\n",
    "\n",
    "plt.xlabel(\"Lp Coefficient\")\n",
    "plt.ylabel(\"p norm\")\n",
    "plt.title(\"Similarity Concentration (Using Cosine-sim Matrix)\")\n",
    "\n",
    "plt.xticks(range(len(cfg.lp_alphas)), cfg.lp_alphas, rotation=30)\n",
    "plt.yticks(\n",
    "    range(len(cfg.ps())),\n",
    "    cfg.ps(),\n",
    ")\n",
    "plt.savefig(f\"images_{cfg.seed}/cosim.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use either sim_conc_cosim or sim_conc_co_occur\n",
    "# plt.imshow(sim_conc_cosim[...,0]) #.mean(dim=-1))\n",
    "plt.imshow(sim_conc_co_occur[..., 0])\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Lp Coefficient\")\n",
    "plt.ylabel(\"p norm\")\n",
    "plt.title(\"Similarity Concentration (Using Co-occurence Matrix)\")\n",
    "\n",
    "plt.xticks(range(len(cfg.lp_alphas)), cfg.lp_alphas, rotation=30)\n",
    "plt.yticks(\n",
    "    range(len(cfg.ps())),\n",
    "    cfg.ps(),\n",
    ")\n",
    "plt.savefig(f\"images_{cfg.seed}/co_occur.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b7399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use either sim_conc_cosim or sim_conc_co_occur\n",
    "# plt.imshow(sim_conc_cosim[...,0]) #.mean(dim=-1))\n",
    "plt.imshow(sim_conc_corr[..., 0])\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Lp Coefficient\")\n",
    "plt.ylabel(\"p norm\")\n",
    "plt.title(\"Similarity Concentration (Using Correlation Matrix)\")\n",
    "\n",
    "plt.xticks(range(len(cfg.lp_alphas)), cfg.lp_alphas, rotation=30)\n",
    "plt.yticks(\n",
    "    range(len(cfg.ps())),\n",
    "    cfg.ps(),\n",
    ")\n",
    "plt.savefig(f\"images_{cfg.seed}/corr.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06878708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For topk, seed 14 with opt_topk\n",
    "for matrix in [mmcs, sim_conc_cosim, sim_conc_co_occur, sim_conc_corr]:\n",
    "    if matrix.shape[-1] == 3:\n",
    "        matrix = matrix[..., 0]\n",
    "    matrix_max = torch.max(\n",
    "        matrix.nan_to_num(0),\n",
    "    )\n",
    "    max_arg = (matrix == matrix_max).nonzero()[0]\n",
    "    p_id, lp_id = max_arg\n",
    "    print(f\"Best p={cfg.ps()[p_id]}, alpha={cfg.lp_alphas[lp_id]}\")\n",
    "    print(\n",
    "        f\"     Metric: {matrix_max: .8f} \\\n",
    "          Reconstruction: {final_reconstruction[p_id][lp_id]: .4E}\\\n",
    "          L2 Ratio: {l2_ratio[p_id][lp_id]: .4f}\"\n",
    "    )\n",
    "# For topk, seed 14 with opt_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ddd5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For topk, seed 14\n",
    "for matrix in [mmcs, sim_conc_cosim, sim_conc_co_occur, sim_conc_corr]:\n",
    "    if matrix.shape[-1] == 3:\n",
    "        matrix = matrix[..., 0]\n",
    "    matrix_max = torch.max(\n",
    "        matrix.nan_to_num(0),\n",
    "    )\n",
    "    max_arg = (matrix == matrix_max).nonzero()[0]\n",
    "    p_id, lp_id = max_arg\n",
    "    print(f\"Best p={cfg.ps()[p_id]}, alpha={cfg.lp_alphas[lp_id]}\")\n",
    "    print(\n",
    "        f\"     Metric: {matrix_max: .8f} \\\n",
    "          Reconstruction: {final_reconstruction[p_id][lp_id]: .4E}\\\n",
    "          L2 Ratio: {l2_ratio[p_id][lp_id]: .4f}\"\n",
    "    )\n",
    "# For topk, seed 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce7be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For topk, seed 15\n",
    "for matrix in [mmcs, sim_conc_cosim, sim_conc_co_occur, sim_conc_corr]:\n",
    "    if matrix.shape[-1] == 3:\n",
    "        matrix = matrix[..., 0]\n",
    "    matrix_max = torch.max(\n",
    "        matrix.nan_to_num(0),\n",
    "    )\n",
    "    max_arg = (matrix == matrix_max).nonzero()[0]\n",
    "    p_id, lp_id = max_arg\n",
    "    print(f\"Best p={cfg.ps()[p_id]}, alpha={cfg.lp_alphas[lp_id]}\")\n",
    "    print(\n",
    "        f\"     Metric: {matrix_max: .8f} \\\n",
    "          Reconstruction: {final_reconstruction[p_id][lp_id]: .4E}\\\n",
    "          L2 Ratio: {l2_ratio[p_id][lp_id]: .4f}\"\n",
    "    )\n",
    "    # For topk, seed 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba46b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For lp_norm, seed 12\n",
    "for matrix in [mmcs, sim_conc_cosim, sim_conc_co_occur, sim_conc_corr]:\n",
    "    if matrix.shape[-1] == 3:\n",
    "        matrix = matrix[..., 0]\n",
    "    matrix_max = torch.max(\n",
    "        matrix.nan_to_num(0),\n",
    "    )\n",
    "    max_arg = (matrix == matrix_max).nonzero()[0]\n",
    "    p_id, lp_id = max_arg\n",
    "    print(f\"Best p={cfg.ps()[p_id]}, alpha={cfg.lp_alphas[lp_id]}\")\n",
    "    print(\n",
    "        f\"     Metric: {matrix_max: .8f} \\\n",
    "          Reconstruction: {final_reconstruction[p_id][lp_id]: .4E}\\\n",
    "          L2 Ratio: {l2_ratio[p_id][lp_id]: .4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967c9d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For lp_norm, seed 6\n",
    "for matrix in [mmcs, sim_conc_cosim, sim_conc_co_occur, sim_conc_corr]:\n",
    "    if matrix.shape[-1] == 3:\n",
    "        matrix = matrix[..., 0]\n",
    "    matrix_max = torch.max(\n",
    "        matrix.nan_to_num(0),\n",
    "    )\n",
    "    max_arg = (matrix == matrix_max).nonzero()[0]\n",
    "    p_id, lp_id = max_arg\n",
    "    print(f\"Best p={cfg.ps[p_id]}, alpha={cfg.lp_alphas[lp_id]}\")\n",
    "    print(\n",
    "        f\"     Metric: {matrix_max: .8f} \\\n",
    "          Reconstruction: {final_reconstruction[p_id][lp_id]: .4E}\\\n",
    "          L2 Ratio: {l2_ratio[p_id][lp_id]: .4f}\"\n",
    "    )\n",
    "    # seed 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40a1b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_id = 4\n",
    "lp_id = 6\n",
    "print(f\"Best p={cfg.ps[p_id]}, alpha={cfg.lp_alphas[lp_id]}\")\n",
    "print(\n",
    "    f\"     Metric: {matrix_max: .8f} \\\n",
    "        Reconstruction: {final_reconstruction[p_id][lp_id]: .4E}\\\n",
    "        L2 Ratio: {l2_ratio[p_id][lp_id]: .4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1a569e",
   "metadata": {},
   "source": [
    "# Examining just the L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3327dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metrics for each layer\n",
    "\n",
    "for p_id in range(len(cfg.ps())):\n",
    "    p_name = cfg.ps()[p_id]\n",
    "    # print(f\"using p of {p_name}\")\n",
    "    matrix = mmcs[p_id] #sim_conc_cosim[p_id] mmcs\n",
    "    if matrix.shape[-1] == 3:\n",
    "        matrix = matrix[..., 0]\n",
    "    matrix_max = torch.max(\n",
    "        matrix.nan_to_num(0),\n",
    "    )\n",
    "    max_arg = (matrix == matrix_max).nonzero()[0]\n",
    "    lp_id = max_arg\n",
    "\n",
    "    print(f\"Given p={cfg.ps()[p_id]}, best alpha={cfg.lp_alphas[lp_id]}\")\n",
    "    print(\n",
    "        f\"     Metric: {matrix_max.item(): .8f} \\\n",
    "          Reconstruction: {final_reconstruction[p_id][lp_id].item(): .4E}\\\n",
    "          L2 Ratio: {l2_ratio[p_id][lp_id].item(): .4f}\"\n",
    "    )\n",
    "    # l1_sae = auto_encoders[p_id][lp_id]\n",
    "    # alive = get_alive_neurons(l1_sae, data_generator, n_batches=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b27715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best L1 coeff\n",
    "\n",
    "for p_id in [-2]:\n",
    "    p_name = cfg.ps()[p_id]\n",
    "    print(f\"using p of {p_name}\")\n",
    "    matrix = mmcs[p_id]\n",
    "    if matrix.shape[-1] == 3:\n",
    "        matrix = matrix[..., 0]\n",
    "    matrix_max = torch.max(\n",
    "        matrix.nan_to_num(0),\n",
    "    )\n",
    "    max_arg = (matrix == matrix_max).nonzero()[0]\n",
    "    lp_id = max_arg\n",
    "\n",
    "    print(f\"Given p={cfg.ps()[p_id]}, best alpha={cfg.lp_alphas[lp_id]}\")\n",
    "    print(\n",
    "        f\"     Metric: {matrix_max.item(): .8f} \\\n",
    "          Reconstruction: {final_reconstruction[p_id][lp_id].item(): .4E}\\\n",
    "          L2 Ratio: {l2_ratio[p_id][lp_id].item(): .4f}\"\n",
    "    )\n",
    "    l1_sae = auto_encoders[p_id][lp_id]\n",
    "    alive = get_alive_neurons(l1_sae, data_generator, n_batches=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f12bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best L0.6 coeff\n",
    "\n",
    "for p_id in [5]:\n",
    "    p_name = cfg.ps()[p_id]\n",
    "    print(f\"using p of {p_name}\")\n",
    "    matrix = mmcs[p_id]\n",
    "    if matrix.shape[-1] == 3:\n",
    "        matrix = matrix[..., 0]\n",
    "    matrix_max = torch.max(\n",
    "        matrix.nan_to_num(0),\n",
    "    )\n",
    "    max_arg = (matrix == matrix_max).nonzero()[0]\n",
    "    lp_id = max_arg\n",
    "\n",
    "    print(f\"Given p={cfg.ps()[p_id]}, best alpha={cfg.lp_alphas[lp_id]}\")\n",
    "    print(\n",
    "        f\"     Metric: {matrix_max.item(): .8f} \\\n",
    "          Reconstruction: {final_reconstruction[p_id][lp_id].item(): .4E}\\\n",
    "          L2 Ratio: {l2_ratio[p_id][lp_id].item(): .4f}\"\n",
    "    )\n",
    "    l1_sae = auto_encoders[p_id][lp_id]\n",
    "    alive = get_alive_neurons(l1_sae, data_generator, n_batches=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12970629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best L0.1^p coeff\n",
    "\n",
    "for p_id in [0]:\n",
    "    p_name = cfg.ps()[p_id]\n",
    "    print(f\"using p of {p_name}\")\n",
    "    matrix = mmcs[p_id]\n",
    "    if matrix.shape[-1] == 3:\n",
    "        matrix = matrix[..., 0]\n",
    "    matrix_max = torch.max(\n",
    "        matrix.nan_to_num(0),\n",
    "    )\n",
    "    max_arg = (matrix == matrix_max).nonzero()[0]\n",
    "    lp_id = max_arg\n",
    "\n",
    "    print(f\"Given p={cfg.ps()[p_id]}, best alpha={cfg.lp_alphas[lp_id]}\")\n",
    "    print(\n",
    "        f\"     Metric: {matrix_max.item(): .8f} \\\n",
    "          Reconstruction: {final_reconstruction[p_id][lp_id].item(): .4E}\\\n",
    "          L2 Ratio: {l2_ratio[p_id][lp_id].item(): .4f}\"\n",
    "    )\n",
    "    l1_sae = auto_encoders[p_id][lp_id]\n",
    "    alive = get_alive_neurons(l1_sae, data_generator, n_batches=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e32e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature corrs\n",
    "auto_encoder = l1_sae\n",
    "\n",
    "# adjacency_matrix[ground_truth_feature,sae_feature] = number of times they co-occur\n",
    "adjacency_matrix = torch.zeros(\n",
    "    (cfg.n_ground_truth_components, cfg.n_components_dictionary), device=device\n",
    ")\n",
    "adjacency_matrix_3d = torch.zeros(\n",
    "    (cfg.n_ground_truth_components, cfg.n_ground_truth_components, cfg.n_components_dictionary), \n",
    "    device=device\n",
    ")\n",
    "auto_adjacency_matrix = torch.zeros(\n",
    "    (cfg.n_components_dictionary, cfg.n_components_dictionary), device=device\n",
    ")\n",
    "\n",
    "# iteratively calculate correlations\n",
    "corr_calculator = BatchCorrelationCalculator(\n",
    "    cfg.n_ground_truth_components, cfg.n_components_dictionary, device=cfg.device\n",
    ")\n",
    "\n",
    "feat_corr_calculator = BatchCorrelationCalculator(\n",
    "    cfg.n_components_dictionary, cfg.n_components_dictionary, device=cfg.device\n",
    ")\n",
    "\n",
    "sae_feats = nn.functional.normalize(\n",
    "    auto_encoder.decoder.weight.data, dim=0\n",
    ")\n",
    "cosim_matrix = data_generator.feats @ sae_feats  # of shape (ground_feats, sae_feats)\n",
    "f_cosim_matrix = sae_feats.mT @ sae_feats # of shape (sae_feats, sae_feats)\n",
    "# mmcs[p_id, lp_id] = cosim_matrix.max(dim=-1)[0].mean()\n",
    "# might need to flip data_generator.feats?\n",
    "\n",
    "ground_truth_appearances = torch.zeros((cfg.n_ground_truth_components), device=device)\n",
    "sae_appearances = torch.zeros((cfg.n_components_dictionary), device=device)\n",
    "\n",
    "g_or_appearances = torch.zeros((cfg.n_ground_truth_components, cfg.n_ground_truth_components), device=device)\n",
    "g_or_adjacency = torch.zeros((cfg.n_ground_truth_components, cfg.n_ground_truth_components, cfg.n_components_dictionary), device=device)\n",
    "\n",
    "g_and_appearances = torch.zeros((cfg.n_ground_truth_components, cfg.n_ground_truth_components), device=device)\n",
    "g_and_adjacency = torch.zeros((cfg.n_ground_truth_components, cfg.n_ground_truth_components, cfg.n_components_dictionary), device=device)\n",
    "\n",
    "inp_norm = 0\n",
    "outp_norm = 0\n",
    "\n",
    "num_inner_epochs = 10000\n",
    "for epoch in tqdm(range(num_inner_epochs)):  # range(cfg.epochs):\n",
    "    ground_truth, batch = next(data_generator)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        x_hat1, c1 = auto_encoder(batch)\n",
    "        # x_hat2, c2 = auto_encoder2(Mbatch)\n",
    "        inp_norm += batch.norm(p=2, dim=-1).mean()\n",
    "        outp_norm += x_hat1.norm(p=2, dim=-1).mean()\n",
    "        \n",
    "        corr_calculator.update(ground_truth, c1) # calc corr before binarizing\n",
    "        feat_corr_calculator.update(c1, c1)\n",
    "\n",
    "        ground_truth[ground_truth != 0] = 1\n",
    "        c1[c1 != 0] = 1\n",
    "\n",
    "        adjacency_matrix += torch.einsum(\"bg, bs -> gs\", ground_truth, c1)\n",
    "        auto_adjacency_matrix += torch.einsum(\"bg, bs -> gs\", c1, c1)\n",
    "        adjacency_matrix_3d += torch.einsum(\"bg, bh, bs -> ghs\", ground_truth, ground_truth, c1)\n",
    "        \n",
    "        g_or = 1-torch.einsum(\"bg, bs -> bgs\", 1-ground_truth, 1-ground_truth)\n",
    "        g_and = torch.einsum(\"bg, bs -> bgs\", ground_truth, ground_truth)\n",
    "        g_or_appearances += g_or.sum(dim=0)\n",
    "        g_and_appearances += g_and.sum(dim=0)\n",
    "        \n",
    "        g_or_adjacency += torch.einsum(\"bgs, bf -> gsf\", g_or, c1)\n",
    "        g_and_adjacency += torch.einsum(\"bgs, bf -> gsf\", g_and, c1)\n",
    "        \n",
    "        ground_truth_appearances += ground_truth.sum(dim=0)\n",
    "        sae_appearances += c1.sum(dim=0)\n",
    "\n",
    "g_corr_matrix = corr_calculator.compute_correlation()\n",
    "f_corr_matrix = feat_corr_calculator.compute_correlation()\n",
    "\n",
    "# compute union (A or B) by PIE\n",
    "union = ground_truth_appearances[:,None] + sae_appearances[None, :] - adjacency_matrix\n",
    "auto_union = sae_appearances[:,None] + sae_appearances[None, :] - auto_adjacency_matrix\n",
    "\n",
    "jaccard = (adjacency_matrix/union)\n",
    "auto_jaccard = (auto_adjacency_matrix/auto_union)\n",
    "\n",
    "# compute union_3d [(g_A or g_B) and f_C]=[(g_A and f_C) or (g_B and f_C)] by PIE\n",
    "union_3d = adjacency_matrix.unsqueeze(0) + adjacency_matrix.unsqueeze(1) - adjacency_matrix_3d\n",
    "jaccard_3d = (adjacency_matrix_3d/union_3d)\n",
    "\n",
    "union_combo = g_and_appearances[:,:,None] + sae_appearances[None, None, :] - g_and_adjacency\n",
    "jaccard_combo = union_combo/g_and_adjacency\n",
    "\n",
    "union_or = g_or_appearances[:,:,None] + sae_appearances[None, None, :] - g_or_adjacency\n",
    "jaccard_or = union_or/g_or_adjacency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b04e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01c99b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_or_adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93c5560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical Clustering\n",
    "def seriation(Z,N,cur_index):\n",
    "    '''\n",
    "        input:\n",
    "            - Z is a hierarchical tree (dendrogram)\n",
    "            - N is the number of points given to the clustering process\n",
    "            - cur_index is the position in the tree for the recursive traversal\n",
    "        output:\n",
    "            - order implied by the hierarchical tree Z\n",
    "            \n",
    "        seriation computes the order implied by a hierarchical tree (dendrogram)\n",
    "    '''\n",
    "    if cur_index < N:\n",
    "        return [cur_index]\n",
    "    else:\n",
    "        left = int(Z[cur_index-N,0])\n",
    "        right = int(Z[cur_index-N,1])\n",
    "        return (seriation(Z,N,left) + seriation(Z,N,right))\n",
    "    \n",
    "def compute_serial_matrix(dist_mat,method=\"ward\"):\n",
    "    '''\n",
    "        input:\n",
    "            - dist_mat is a distance matrix\n",
    "            - method = [\"ward\",\"single\",\"average\",\"complete\"]\n",
    "        output:\n",
    "            - seriated_dist is the input dist_mat,\n",
    "              but with re-ordered rows and columns\n",
    "              according to the seriation, i.e. the\n",
    "              order implied by the hierarchical tree\n",
    "            - res_order is the order implied by\n",
    "              the hierarhical tree\n",
    "            - res_linkage is the hierarhical tree (dendrogram)\n",
    "        \n",
    "        compute_serial_matrix transforms a distance matrix into \n",
    "        a sorted distance matrix according to the order implied \n",
    "        by the hierarchical tree (dendrogram)\n",
    "    '''\n",
    "    N = len(dist_mat)\n",
    "    flat_dist_mat = dist_mat #squareform(dist_mat)\n",
    "    res_linkage = linkage(flat_dist_mat, method=method,preserve_input=True)\n",
    "    res_order = seriation(res_linkage, N, N + N-2)\n",
    "    return res_order, res_linkage\n",
    "\n",
    "def sort_cross_similarity(similarity_mat):\n",
    "    '''\n",
    "        Given a non-square matrix that is 1 for highly similar variables \n",
    "        and 0 for dis-similar variables, sort just the columns.\n",
    "        Shape ground_truth x sae_features\n",
    "    '''\n",
    "    most_similar_gt_feature = np.argmax(similarity_mat, axis=0)\n",
    "    similarity_strength = np.max(similarity_mat, axis=0)\n",
    "\n",
    "    # Step 2: Sort the experimental features based on the identified ground truth feature and similarity strength\n",
    "    # We'll create a structured array for sorting\n",
    "    dtype = [('gt_index', int), ('similarity', float), ('exp_index', int)]\n",
    "    data = np.array([(gt_index, -similarity_strength[i], i) for i, gt_index in enumerate(most_similar_gt_feature)], dtype=dtype)\n",
    "\n",
    "    # Sorting - Primary by gt_index then by similarity strength\n",
    "    sorted_data = np.sort(data, order=['gt_index', 'similarity'])\n",
    "\n",
    "    # Extracting the sorted experimental feature indices\n",
    "    sorted_exp_indices = sorted_data['exp_index']\n",
    "    return sorted_exp_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd9ed65",
   "metadata": {},
   "source": [
    "Compute hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd1cb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_matrix = f_corr_matrix[alive][:,alive].cpu() #cosim_matrix\n",
    "\n",
    "cosim_matrix = (\n",
    "    (\n",
    "        nn.functional.normalize(l1_sae.decoder.weight.data[:,alive], dim=0).mT\n",
    "        @ nn.functional.normalize(l1_sae.decoder.weight.data[:,alive], dim=0)\n",
    "    )\n",
    "    .detach()\n",
    "    .cpu()\n",
    ")\n",
    "cosim_matrix_g = (\n",
    "    (\n",
    "        data_generator.feats\n",
    "        @ nn.functional.normalize(l1_sae.decoder.weight.data[:,alive], dim=0)\n",
    "    )\n",
    "    .detach()\n",
    "    .cpu()\n",
    ")\n",
    "\n",
    "use_matrix = g_corr_matrix[:,alive] #cosim_matrix_g\n",
    "\n",
    "# methods = [\"average\"] #[\"ward\",\"single\",\"average\",\"complete\"]\n",
    "# for method in methods:\n",
    "#     print(\"Method:\\t\",method)\n",
    "    \n",
    "res_order = sort_cross_similarity(use_matrix.cpu().numpy())\n",
    "sorted_cosim_matrix_g = cosim_matrix_g[:, res_order]\n",
    "sorted_cosim_matrix = cosim_matrix[res_order, res_order]\n",
    "\n",
    "sorted_corr_g = g_corr_matrix[:,alive][:, res_order].cpu()\n",
    "sorted_corr_f = f_corr_matrix[:,alive][alive,:][res_order,:][:, res_order].cpu()\n",
    "\n",
    "sorted_dirs = nn.functional.normalize(l1_sae.decoder.weight.data[:,alive], dim=0).T[res_order,:]\n",
    "sorted_thresholds = (-l1_sae.encoder[0].bias/l1_sae.encoder[0].weight.norm(dim=1))[alive][res_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b43915",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_jaccard_3d = jaccard_3d[:,:,alive][:,:,res_order].cpu().numpy()\n",
    "sorted_max_jaccard_3d = jaccard_3d[:,:,alive][:,:,res_order].cpu().clone()\n",
    "for i in range(sorted_max_jaccard_3d.shape[0]):\n",
    "    sorted_max_jaccard_3d[i,i,:] = 0\n",
    "sorted_max_jaccard_3d = sorted_max_jaccard_3d.amax(dim=(0,1))\n",
    "\n",
    "sorted_jaccard = jaccard[:,alive][:,res_order].cpu().numpy() \n",
    "sorted_max_jaccard = torch.tensor(sorted_jaccard).amax(dim=0).numpy()\n",
    "sorted_auto_jaccard = auto_jaccard[alive,:][:,alive][:,res_order][res_order,:].cpu().numpy() \n",
    "\n",
    "sorted_jaccard_combo = jaccard_combo[:,:,alive][:,:,res_order].cpu().numpy()\n",
    "sorted_max_jaccard_combo = jaccard_combo[:,:,alive][:,:,res_order].cpu().clone()\n",
    "for i in range(sorted_max_jaccard_combo.shape[0]):\n",
    "    sorted_max_jaccard_combo[i,i,:] = 0\n",
    "sorted_max_jaccard_combo = sorted_max_jaccard_combo.amax(dim=(0,1))\n",
    "\n",
    "sorted_jaccard_or = jaccard_or[:,:,alive][:,:,res_order].cpu().numpy()\n",
    "sorted_max_jaccard_or = jaccard_or[:,:,alive][:,:,res_order].cpu().clone()\n",
    "for i in range(sorted_max_jaccard_or.shape[0]):\n",
    "    sorted_max_jaccard_or[i,i,:] = 0\n",
    "sorted_max_jaccard_or = sorted_max_jaccard_or.amax(dim=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed50b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_thresholds.max(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d0d26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosim_matrix = (\n",
    "    (\n",
    "        nn.functional.normalize(l1_sae.decoder.weight.data[:,alive], dim=0).mT\n",
    "        @ nn.functional.normalize(l1_sae.decoder.weight.data[:,alive], dim=0)\n",
    "    )\n",
    "    .detach()\n",
    "    .cpu()\n",
    ")\n",
    "if cfg.loss_fn==\"lp^p\":\n",
    "    append = \"^p\"\n",
    "else:\n",
    "    append = \"\"\n",
    "    \n",
    "sorted_cosim_matrix = cosim_matrix[res_order][:,res_order] #.sort(descending=True, dim=-1)[0].cpu()\n",
    "plt.imshow(sorted_cosim_matrix, cmap=\"PiYG\", vmin=-1, vmax=1)\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Cosine Similarity', rotation=270)\n",
    "plt.xlabel(\"SAE features\")\n",
    "plt.ylabel(\"SAE features\")\n",
    "plt.title(f\"SAE Features Cosine Similarity (p={p_name})\")\n",
    "plt.savefig(f\"images_{cfg.seed}/ffl{p_name}{append}.png\")\n",
    "\n",
    "\n",
    "pre = f\"metrics_{cfg.seed}\"\n",
    "if not os.path.exists(f\"/root/sparsify/notebooks/{pre}\"):\n",
    "    os.makedirs(f\"/root/sparsify/notebooks/{pre}\")\n",
    "torch.save(sorted_cosim_matrix, f\"{pre}/ffl{p_name}{append}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666d779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosim_matrix = (\n",
    "    (\n",
    "        data_generator.feats\n",
    "        @ nn.functional.normalize(l1_sae.decoder.weight.data[:,alive], dim=0)\n",
    "    )\n",
    "    .detach()\n",
    "    .cpu()\n",
    ")\n",
    "if cfg.loss_fn==\"lp^p\":\n",
    "    append = \"^p\"\n",
    "else:\n",
    "    append = \"\"\n",
    "    \n",
    "sorted_cosim_matrix = cosim_matrix[:][:,res_order] #cosim_matrix.sort(descending=True, dim=-1)[0].cpu()\n",
    "plt.imshow(sorted_cosim_matrix, cmap=\"PiYG\", vmin=-1, vmax=1)  # cosim_matrix.detach().cpu()\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Cosine Similarity', rotation=270)\n",
    "plt.xlabel(\"Sorted SAE features\")\n",
    "plt.ylabel(\"Ground truth features\")\n",
    "plt.title(f\"Ground Truth vs SAE Features Cosine Similarity (p={p_name})\")\n",
    "plt.savefig(f\"images_{cfg.seed}/gfl{p_name}{append}.png\")\n",
    "\n",
    "# plt.xticks(range(len(cfg.lp_alphas)), cfg.lp_alphas, rotation=30)\n",
    "# plt.yticks(\n",
    "#     range(len(cfg.ps)),\n",
    "#     cfg.ps,\n",
    "# )\n",
    "\n",
    "pre = f\"metrics_{cfg.seed}\"\n",
    "if not os.path.exists(f\"/root/sparsify/notebooks/{pre}\"):\n",
    "    os.makedirs(f\"/root/sparsify/notebooks/{pre}\")\n",
    "torch.save(sorted_cosim_matrix, f\"{pre}/gfl{p_name}{append}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b51fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.loss_fn==\"lp^p\":\n",
    "    append = \"^p\"\n",
    "else:\n",
    "    append = \"\"\n",
    "    \n",
    "plt.imshow(sorted_corr_f, cmap=\"PiYG\", vmin=-1, vmax=1)\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Correlation', rotation=270)\n",
    "plt.xlabel(\"SAE features\")\n",
    "plt.ylabel(\"SAE features\")\n",
    "plt.title(f\"SAE Features Correlation (p={p_name})\")\n",
    "plt.savefig(f\"images_{cfg.seed}/ffl{p_name}{append}_corr.png\")\n",
    "\n",
    "\n",
    "pre = f\"metrics_{cfg.seed}\"\n",
    "if not os.path.exists(f\"/root/sparsify/notebooks/{pre}\"):\n",
    "    os.makedirs(f\"/root/sparsify/notebooks/{pre}\")\n",
    "torch.save(sorted_corr_f, f\"{pre}/ffl{p_name}{append}_corr.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb1e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.loss_fn==\"lp^p\":\n",
    "    append = \"^p\"\n",
    "else:\n",
    "    append = \"\"\n",
    "    \n",
    "plt.imshow(sorted_corr_g, cmap=\"PiYG\", vmin=-1, vmax=1)\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Correlation', rotation=270)\n",
    "plt.xlabel(\"SAE features\")\n",
    "plt.ylabel(\"Ground truth features\")\n",
    "plt.title(f\"Ground Truth vs SAE Features Correlation (p={p_name})\")\n",
    "plt.savefig(f\"images_{cfg.seed}/gfl{p_name}{append}_corr.png\")\n",
    "\n",
    "\n",
    "pre = f\"metrics_{cfg.seed}\"\n",
    "if not os.path.exists(f\"/root/sparsify/notebooks/{pre}\"):\n",
    "    os.makedirs(f\"/root/sparsify/notebooks/{pre}\")\n",
    "torch.save(sorted_corr_g, f\"{pre}/gfl{p_name}{append}_corr.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b62c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D jaccard\n",
    "# plt.imshow(sorted_max_jaccard_3d, cmap=\"PiYG\", vmin=-1, vmax=1)\n",
    "if cfg.loss_fn==\"lp^p\":\n",
    "    append = \"^p\"\n",
    "else:\n",
    "    append = \"\"\n",
    "    \n",
    "plt.plot(sorted_max_jaccard, label=\"Max Jaccard\")\n",
    "plt.plot(sorted_max_jaccard_combo, label=\"Max Jaccard with Combo\")\n",
    "plt.plot(sorted_max_jaccard_or, label=\"Max Jaccard with Or\")\n",
    "plt.legend()\n",
    "plt.title(f\"Max Ground-Ground Jaccard Conditional on SAE Feature, p={p_name}\")\n",
    "# cbar = plt.colorbar()\n",
    "# cbar.ax.set_ylabel('Cosine Similarity', rotation=270, labelpad=15)\n",
    "\n",
    "# plt.imshow(sorted_jaccard_3d[:,:,11], cmap=\"PiYG\", vmin=-1, vmax=1)\n",
    "# cbar = plt.colorbar()\n",
    "# cbar.ax.set_ylabel('Cosine Similarity', rotation=270, labelpad=15)\n",
    "\n",
    "pre = f\"metrics_{cfg.seed}\"\n",
    "if not os.path.exists(f\"/root/sparsify/notebooks/{pre}\"):\n",
    "    os.makedirs(f\"/root/sparsify/notebooks/{pre}\")\n",
    "torch.save(sorted_jaccard_3d, f\"{pre}/jaccard_3d{p_name}{append}.pt\")\n",
    "torch.save(sorted_max_jaccard_3d, f\"{pre}/max_jaccard_3d{p_name}{append}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3353eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_combo_index = torch.max(sorted_max_jaccard_3d, dim=0)[1]\n",
    "re_sorted, indices = sorted_max_jaccard_3d.sort()\n",
    "print(re_sorted)\n",
    "print(indices)\n",
    "print(sorted_thresholds[indices[-1]].item(), sorted_thresholds[indices[-2]].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fe252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(list(sorted_jaccard[:,most_combo_index]))[::-1])\n",
    "print(sorted(list(sorted_cosim_matrix[:,most_combo_index].numpy()))[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd691ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_jaccard_3d[:,3].max()\n",
    "plt.imshow(sorted_jaccard_3d[:,:,17], cmap=\"PiYG\", vmin=-1, vmax=1)\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Cosine Similarity', rotation=270, labelpad=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90293d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sorted_jaccard[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa273c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.loss_fn==\"lp^p\":\n",
    "    append = \"^p\"\n",
    "else:\n",
    "    append = \"\"\n",
    "plt.imshow(sorted_jaccard, cmap=\"PiYG\", vmin=-1, vmax=1)  # cosim_matrix.detach().cpu()\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Cosine Similarity', rotation=270)\n",
    "plt.xlabel(\"Sorted SAE features\")\n",
    "plt.ylabel(\"Ground truth features\")\n",
    "plt.title(f\"Ground Truth vs SAE Features Jaccard Index (p={p_name})\")\n",
    "plt.savefig(f\"images_{cfg.seed}/gfl{p_name}{append}_jaccard.png\")\n",
    "\n",
    "\n",
    "pre = f\"metrics_{cfg.seed}\"\n",
    "if not os.path.exists(f\"/root/sparsify/notebooks/{pre}\"):\n",
    "    os.makedirs(f\"/root/sparsify/notebooks/{pre}\")\n",
    "torch.save(sorted_jaccard, f\"{pre}/gfl{p_name}{append}_jaccard.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d55e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sorted_auto_jaccard, cmap=\"PiYG\", vmin=-1, vmax=1)  # cosim_matrix.detach().cpu()\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Cosine Similarity', rotation=270)\n",
    "plt.xlabel(\"Sorted SAE features\")\n",
    "plt.ylabel(\"Sorted SAE features\")\n",
    "plt.title(f\"SAE Features Jaccard Index (p={p_name})\")\n",
    "plt.savefig(f\"images_{cfg.seed}/ffl{p_name}{append}_jaccard.png\")\n",
    "\n",
    "\n",
    "pre = f\"metrics_{cfg.seed}\"\n",
    "if not os.path.exists(f\"/root/sparsify/notebooks/{pre}\"):\n",
    "    os.makedirs(f\"/root/sparsify/notebooks/{pre}\")\n",
    "torch.save(sorted_auto_jaccard, f\"{pre}/ffl{p_name}{append}_jaccard.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2672e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(g_corr_matrix[:,alive][:,res_order].cpu(), cmap=\"PiYG\", vmin=-1, vmax=1)\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Activation Correlation', rotation=270, labelpad=15)\n",
    "plt.xlabel(\"SAE features\")\n",
    "plt.ylabel(\"Ground truth features\")\n",
    "plt.title(f\"SAE Feature Correlation (L{p_name})\")\n",
    "plt.savefig(f\"images_{cfg.seed}/gfl{p_name}_correlation.png\")\n",
    "\n",
    "# plt.xticks(range(len(cfg.lp_alphas)), cfg.lp_alphas, rotation=30)\n",
    "# plt.yticks(\n",
    "#     range(len(cfg.ps)),\n",
    "#     cfg.ps,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51895b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_feature(sae, f, alive=None):\n",
    "#     # f is the feature id\n",
    "#     # returns encoder direction, decoder direction, enc bias, decoder bias\n",
    "#     # if alive is not None, take the f'th alive feature\n",
    "#     if alive is not None:\n",
    "        \n",
    "    \n",
    "#     return sae.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e77f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_corr_matrix[alive][:,alive].abs().sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dbe7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_corr_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8433e770",
   "metadata": {},
   "outputs": [],
   "source": [
    "alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e3b6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lee-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
