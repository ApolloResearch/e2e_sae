{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting natsort\n",
      "  Using cached natsort-8.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Using cached natsort-8.4.0-py3-none-any.whl (38 kB)\n",
      "Installing collected packages: natsort\n",
      "Successfully installed natsort-8.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jordan/miniconda3/envs/py312/lib/python3.12/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from typing import TYPE_CHECKING\n",
    "if TYPE_CHECKING:\n",
    "    from transformer_lens import HookedTransformer\n",
    "from natsort import natsorted\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "from sparsify.models.transformers import SAETransformer\n",
    "from sparsify.log import logger\n",
    "from sparsify.utils import filter_names, load_config\n",
    "from sparsify.data import DatasetConfig\n",
    "from sparsify.loader import load_tlens_model, load_pretrained_saes\n",
    "from sparsify.scripts.train_tlens_saes.run_train_tlens_saes import Config\n",
    "from sparsify.scripts.train_tlens_saes.run_train_tlens_saes import main as run_train\n",
    "from sparsify.scripts.generate_dashboards import DashboardsConfig, PromptDashboardsConfig, generate_dashboards\n",
    "from sparsify.utils import replace_pydantic_model\n",
    "\n",
    "current_dir = Path(os.getcwd())\n",
    "sae_save_dir = Path(current_dir) / Path(\"out\") / Path(\"run_for_testing_feature_dashboards\")\n",
    "dashboard_data_save_dir = sae_save_dir / Path(\"feature_dashboard_data\")\n",
    "sae_position_name = \"blocks.1.hook_resid_post\"\n",
    "config_dir = Path('../sparsify/scripts/train_tlens_saes/tinystories_1M.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Config\nloss.inp_to_out\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'coeff': 1.0}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.6/v/extra_forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m\n\u001b[1;32m      8\u001b[0m     base_config \u001b[38;5;241m=\u001b[39m Config(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39myaml\u001b[38;5;241m.\u001b[39msafe_load(stream\u001b[38;5;241m=\u001b[39mf))\n\u001b[1;32m      9\u001b[0m update_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: sae_save_dir,\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_every_n_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m20000\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwandb_project\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m         }\n\u001b[0;32m---> 22\u001b[0m new_config \u001b[38;5;241m=\u001b[39m \u001b[43mreplace_pydantic_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(new_config)\n\u001b[1;32m     24\u001b[0m run_train(new_config)\n",
      "File \u001b[0;32m/mnt/c/Users/nadro/Documents/AI_safety/MATS5/Sparsify/sparsify/sparsify/utils.py:136\u001b[0m, in \u001b[0;36mreplace_pydantic_model\u001b[0;34m(model, *updates)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplace_pydantic_model\u001b[39m(model: BaseModelType, \u001b[38;5;241m*\u001b[39mupdates: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseModelType:\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a new model with (potentially nested) updates in the form of dictionaries.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m        Bar(foo=Foo(a=3, b=2))\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdeep_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mupdates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/pydantic/main.py:171\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    170\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Config\nloss.inp_to_out\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'coeff': 1.0}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.6/v/extra_forbidden"
     ]
    }
   ],
   "source": [
    "# Train a SAE on tiny-stories-1M and save it to sae_save_dir \n",
    "# Just load a pretrained SAE if we already have one handy\n",
    "if len(list(sae_save_dir.glob(\"*.pt\"))) > 0 or len(list(sae_save_dir.glob(\"*/*.pt\"))) > 0 or len(list(sae_save_dir.glob(\"*\\\\*.pt\"))) > 0: \n",
    "    print(f\"SAEs already exist in sae_save_dir = {sae_save_dir}\\nUsing those.\")\n",
    "else:\n",
    "    # Train a shitty SAE if we don't have one already\n",
    "    with open(config_dir) as f:\n",
    "        base_config = Config(**yaml.safe_load(stream=f))\n",
    "    update_dict = {\n",
    "                \"save_dir\": sae_save_dir,\n",
    "                \"save_every_n_samples\": 20000,\n",
    "                \"n_samples\": 20000,\n",
    "                \"warmup_samples\": 5000,\n",
    "                \"cooldown_samples\": 5000,\n",
    "                \"loss\": {\n",
    "                    \"out_to_in\":{\"coeff\":1.0},\n",
    "                    \"logits_kl\": None\n",
    "                    },\n",
    "                \"saes\": {\"sae_positions\": sae_position_name},\n",
    "                \"wandb_project\": None,\n",
    "            }\n",
    "    new_config = replace_pydantic_model(base_config, update_dict)\n",
    "    print(new_config)\n",
    "    run_train(new_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb_project='tinystories-1m_play' wandb_run_name=None wandb_run_name_prefix='' seed=0 tlens_model_name='roneneldan/TinyStories-1M' tlens_model_path=None save_dir=PosixPath('/mnt/c/Users/nadro/Documents/AI_safety/MATS5/Sparsify/sparsify/sparsify/scripts/train_tlens_saes/out') n_samples=900000 save_every_n_samples=None eval_every_n_samples=10000 eval_n_samples=500 batch_size=10 effective_batch_size=10 lr=0.001 adam_beta1=0.0 warmup_samples=50000 cooldown_samples=200000 max_grad_norm=1.0 log_every_n_grad_steps=20 collect_act_frequency_every_n_samples=10000 act_frequency_n_tokens=500000 collect_output_metrics_every_n_samples=0 loss=LossConfigs(sparsity=SparsityLoss(coeff=0.1, p_norm=1.0), in_to_orig=InToOrigLoss(coeff=0.0, hook_positions=['hook_resid_post']), out_to_orig=None, out_to_in=OutToInLoss(coeff=0.0), logits_kl=LogitsKLLoss(coeff=1.0)) train_data=DatasetConfig(dataset_name='apollo-research/roneneldan-TinyStories-tokenizer-gpt2', is_tokenized=True, tokenizer_name='gpt2', streaming=True, split='train', n_ctx=512, seed=0, column_name='input_ids') eval_data=DatasetConfig(dataset_name='apollo-research/roneneldan-TinyStories-tokenizer-gpt2', is_tokenized=True, tokenizer_name='gpt2', streaming=True, split='validation', n_ctx=512, seed=0, column_name='input_ids') saes=SparsifiersConfig(type_of_sparsifier='sae', dict_size_to_input_ratio=30.0, k=None, pretrained_sae_paths=None, retrain_saes=False, sae_positions=['blocks.1.hook_resid_post'])\n"
     ]
    }
   ],
   "source": [
    "print(base_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved SAEs and the corresponding model\n",
    "def load_SAETransformer_from_saes_path(\n",
    "    saes_path: Path,\n",
    "    config_path: str | Path | None = None,\n",
    "    tlens_model: HookedTransformer | None = None,\n",
    ") -> tuple[SAETransformer, Config, list[str]]:\n",
    "    saes_path = Path(saes_path)\n",
    "    # Allow passing in a directoty and finding the latest .pt or .pth file in it:\n",
    "    if saes_path.suffix != \".pt\" and saes_path.suffix != \".pth\":\n",
    "        if not saes_path.is_dir():\n",
    "            saes_path = saes_path.parent\n",
    "        saes_paths = natsorted(list(saes_path.glob(\"*.pt\")) + list(saes_path.glob(\"*.pth\")))\n",
    "        if len(saes_paths) == 0:\n",
    "            saes_paths = natsorted(list(saes_path.glob(\"*/*.pt\")) + list(saes_path.glob(\"*/*.pth\")))\n",
    "        if len(saes_paths) == 0:\n",
    "            saes_paths = natsorted(list(saes_path.glob(\"*\\\\*.pt\")) + list(saes_path.glob(\"*\\\\*.pth\")))\n",
    "        assert len(saes_paths) > 0, \"Could not find any .pt or .pth files in the saes_path\"\n",
    "        saes_path = saes_paths[-1]\n",
    "    assert saes_path.exists(), \"saes_path does not exist\"\n",
    "    config_path = saes_path.parent / \"config.yaml\" if config_path is None else Path(config_path)\n",
    "    assert (\n",
    "        config_path.exists()\n",
    "    ), \"Could not find the config_path: config.yaml should be in the same folder as the saes_path\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    config = load_config(config_path, config_model=Config)\n",
    "    logger.info(config)\n",
    "    if tlens_model is None:\n",
    "        tlens_model = load_tlens_model(\n",
    "            tlens_model_name=config.tlens_model_name, tlens_model_path=config.tlens_model_path\n",
    "        )\n",
    "    raw_sae_positions = filter_names(\n",
    "        list(tlens_model.hook_dict.keys()), config.saes.sae_positions\n",
    "    )\n",
    "    model = SAETransformer(\n",
    "        config=config, tlens_model=tlens_model, raw_sae_positions=raw_sae_positions\n",
    "    ).to(device=device)\n",
    "\n",
    "    all_param_names = [name for name, _ in model.saes.named_parameters()]\n",
    "    trainable_param_names = load_pretrained_saes(\n",
    "        saes=model.saes,\n",
    "        pretrained_sae_paths=[saes_path]\n",
    "        if config.saes.pretrained_sae_paths is None\n",
    "        else [saes_path] + config.saes.pretrained_sae_paths,\n",
    "        all_param_names=all_param_names,\n",
    "        retrain_saes=config.saes.retrain_saes,\n",
    "    )\n",
    "    return model, config, trainable_param_names\n",
    "\n",
    "print(\"Loading the model and SAEs\")\n",
    "model, config, _ = load_SAETransformer_from_saes_path(sae_save_dir)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = DatasetConfig(\n",
    "    dataset_name='apollo-research/sae-skeskinen-TinyStories-hf-tokenizer-gpt2', \n",
    "    tokenizer_name='gpt2', \n",
    "    split = \"train\",\n",
    "    n_ctx=512, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dashboards\n",
    "dashboards_config = DashboardsConfig(\n",
    "    n_samples = 2000, \n",
    "    batch_size = 20,\n",
    "    minibatch_size_features = 100,\n",
    "    save_dir = sae_save_dir,\n",
    "    data = dataset_config,\n",
    "    feature_indices = list(range(50)),\n",
    "    prompt_centric = PromptDashboardsConfig(\n",
    "        prompts = [\"Sally met Mike at the show. She brought popcorn for him.\"],\n",
    "        n_random_prompt_dashboards = 10\n",
    "    )\n",
    ")\n",
    "generate_dashboards(model, dashboards_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
