{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import TYPE_CHECKING\n",
    "if TYPE_CHECKING:\n",
    "    from transformer_lens import HookedTransformer\n",
    "from IPython.display import display, HTML\n",
    "from natsort import natsorted\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import yaml\n",
    "\n",
    "from sparsify.models.transformers import SAETransformer\n",
    "from sparsify.log import logger\n",
    "from sparsify.utils import filter_names, load_config\n",
    "from sparsify.data import DatasetConfig\n",
    "from sparsify.loader import load_tlens_model, load_pretrained_saes\n",
    "from sparsify.scripts.train_tlens_saes.run_train_tlens_saes import Config\n",
    "from sparsify.scripts.train_tlens_saes.run_train_tlens_saes import main as run_train\n",
    "from sparsify.scripts.generate_dashboards import DashboardsConfig, PromptDashboardsConfig, generate_dashboards\n",
    "from sparsify.utils import replace_pydantic_model\n",
    "\n",
    "current_dir = Path(os.getcwd())\n",
    "sae_save_dir = Path(current_dir) / Path(\"out\") / Path(\"run_for_testing_feature_dashboards\")\n",
    "dashboard_data_save_dir = sae_save_dir / Path(\"feature_dashboard_data\")\n",
    "sae_position_name = \"blocks.1.hook_resid_post\"\n",
    "config_dir = Path('../sparsify/scripts/train_tlens_saes/tinystories_1M.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAEs already exist in sae_save_dir = /mnt/c/Users/nadro/Documents/AI_safety/MATS5/Sparsify/sparsify/notebooks/out/run_for_testing_feature_dashboards\n",
      "Using those.\n"
     ]
    }
   ],
   "source": [
    "# Train a SAE on tiny-stories-1M and save it to sae_save_dir \n",
    "# Just load a pretrained SAE if we already have one handy\n",
    "if len(list(sae_save_dir.glob(\"*.pt\"))) > 0 or len(list(sae_save_dir.glob(\"*/*.pt\"))) > 0 or len(list(sae_save_dir.glob(\"*\\\\*.pt\"))) > 0: \n",
    "    print(f\"SAEs already exist in sae_save_dir = {sae_save_dir}\\nUsing those.\")\n",
    "else:\n",
    "    # Train a shitty SAE if we don't have one already\n",
    "    with open(config_dir) as f:\n",
    "        base_config = Config(**yaml.safe_load(stream=f))\n",
    "    update_dict = {\n",
    "                \"save_dir\": sae_save_dir,\n",
    "                \"save_every_n_samples\": 20000,\n",
    "                \"n_samples\": 20000,\n",
    "                \"warmup_samples\": 5000,\n",
    "                \"cooldown_samples\": 5000,\n",
    "                \"loss\": {\n",
    "                    \"inp_to_out\":{\"coeff\":1.0},\n",
    "                    \"logits_kl\": None\n",
    "                    },\n",
    "                \"saes\": {\"sae_position_names\": sae_position_name},\n",
    "                \"wandb_project\": None,\n",
    "            }\n",
    "    new_config = replace_pydantic_model(base_config, update_dict)\n",
    "    print(new_config)\n",
    "    run_train(new_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model and SAEs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 13:21:57 - INFO - wandb_project=None wandb_run_name=None wandb_run_name_prefix='' seed=0 tlens_model_name='roneneldan/TinyStories-1M' tlens_model_path=None save_dir=PosixPath('/mnt/c/Users/nadro/Documents/AI_safety/MATS5/Sparsify/sparsify/notebooks/run_for_testing_feature_dashboards') n_samples=20000 save_every_n_samples=20000 eval_every_n_samples=10000 eval_n_samples=500 batch_size=10 effective_batch_size=10 lr=0.001 adam_beta1=0.0 warmup_samples=5000 cooldown_samples=5000 max_grad_norm=1.0 log_every_n_grad_steps=20 collect_act_frequency_every_n_samples=10000 act_frequency_n_tokens=500000 collect_output_metrics_every_n_samples=0 loss=LossConfigs(sparsity=SparsityLossConfig(coeff=0.1, p_norm=1.0), inp_to_orig=None, out_to_orig=None, inp_to_out=InpToOutLossConfig(coeff=1.0), logits_kl=None) train_data=DatasetConfig(dataset_name='apollo-research/roneneldan-TinyStories-tokenizer-gpt2', is_tokenized=True, tokenizer_name='gpt2', streaming=True, split='train', n_ctx=512, seed=0, column_name='input_ids') eval_data=DatasetConfig(dataset_name='apollo-research/roneneldan-TinyStories-tokenizer-gpt2', is_tokenized=True, tokenizer_name='gpt2', streaming=True, split='validation', n_ctx=512, seed=0, column_name='input_ids') saes=SparsifiersConfig(type_of_sparsifier='sae', dict_size_to_input_ratio=30.0, k=None, pretrained_sae_paths=None, retrain_saes=False, sae_position_names=['blocks.1.hook_resid_post'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model roneneldan/TinyStories-1M into HookedTransformer\n",
      "Moving model to device:  cuda\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Load the saved SAEs and the corresponding model\n",
    "def load_SAETransformer_from_saes_path(\n",
    "    saes_path: Path,\n",
    "    config_path: str | Path | None = None,\n",
    "    tlens_model: HookedTransformer | None = None,\n",
    ") -> tuple[SAETransformer, Config, list[str]]:\n",
    "    saes_path = Path(saes_path)\n",
    "    # Allow passing in a directoty and finding the latest .pt or .pth file in it:\n",
    "    if saes_path.suffix != \".pt\" and saes_path.suffix != \".pth\":\n",
    "        if not saes_path.is_dir():\n",
    "            saes_path = saes_path.parent\n",
    "        saes_paths = natsorted(list(saes_path.glob(\"*.pt\")) + list(saes_path.glob(\"*.pth\")))\n",
    "        if len(saes_paths) == 0:\n",
    "            saes_paths = natsorted(list(saes_path.glob(\"*/*.pt\")) + list(saes_path.glob(\"*/*.pth\")))\n",
    "        if len(saes_paths) == 0:\n",
    "            saes_paths = natsorted(list(saes_path.glob(\"*\\\\*.pt\")) + list(saes_path.glob(\"*\\\\*.pth\")))\n",
    "        assert len(saes_paths) > 0, \"Could not find any .pt or .pth files in the saes_path\"\n",
    "        saes_path = saes_paths[-1]\n",
    "    assert saes_path.exists(), \"saes_path does not exist\"\n",
    "    config_path = saes_path.parent / \"config.yaml\" if config_path is None else Path(config_path)\n",
    "    assert (\n",
    "        config_path.exists()\n",
    "    ), \"Could not find the config_path: config.yaml should be in the same folder as the saes_path\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    config = load_config(config_path, config_model=Config)\n",
    "    logger.info(config)\n",
    "    if tlens_model is None:\n",
    "        tlens_model = load_tlens_model(\n",
    "            tlens_model_name=config.tlens_model_name, tlens_model_path=config.tlens_model_path\n",
    "        )\n",
    "    raw_sae_position_names = filter_names(\n",
    "        list(tlens_model.hook_dict.keys()), config.saes.sae_position_names\n",
    "    )\n",
    "    model = SAETransformer(\n",
    "        config=config, tlens_model=tlens_model, raw_sae_position_names=raw_sae_position_names\n",
    "    ).to(device=device)\n",
    "\n",
    "    all_param_names = [name for name, _ in model.saes.named_parameters()]\n",
    "    trainable_param_names = load_pretrained_saes(\n",
    "        saes=model.saes,\n",
    "        pretrained_sae_paths=[saes_path]\n",
    "        if config.saes.pretrained_sae_paths is None\n",
    "        else [saes_path] + config.saes.pretrained_sae_paths,\n",
    "        all_param_names=all_param_names,\n",
    "        retrain_saes=config.saes.retrain_saes,\n",
    "    )\n",
    "    return model, config, trainable_param_names\n",
    "\n",
    "print(\"Loading the model and SAEs\")\n",
    "model, config, _ = load_SAETransformer_from_saes_path(sae_save_dir)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = DatasetConfig(\n",
    "    dataset_name='apollo-research/sae-skeskinen-TinyStories-hf-tokenizer-gpt2', \n",
    "    tokenizer_name='gpt2', \n",
    "    split = \"train\",\n",
    "    n_ctx=512, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35753e99e53f42d499c659bd1047501d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daea0c5bc25f4117804beec1fc63de29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature acts:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                | 9/10 [00:05<00:00,  1.55it/s]\n",
      "Parsing activation data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:40<00:00,  3.35s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5472d1f56f2e4a6aaaef8111d1a52c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034fd50db477413c89ca4f793879eb65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Random prompt dashboards: 5it [04:10, 50.06s/it]                                                                                                                                                                                   \n",
      "/home/jordan/miniconda3/envs/py312/lib/python3.12/site-packages/sae_vis/html_fns.py:158: RuntimeWarning: invalid value encountered in divide\n",
      "  bg_values = np.maximum(feat_acts, 0.0) / bg_denom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving HTML feature dashboards for the SAE at blocks.1.hook_resid_post:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dashboard HTML files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 54/54 [00:03<00:00, 14.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved HTML feature dashboards in /mnt/c/Users/nadro/Documents/AI_safety/MATS5/Sparsify/sparsify/notebooks/out/run_for_testing_feature_dashboards/feature-dashboards/dashboards_blocks.1.hook_resid_post\n"
     ]
    }
   ],
   "source": [
    "# Generate the dashboards\n",
    "dashboards_config = DashboardsConfig(\n",
    "    n_samples = 2000, \n",
    "    batch_size = 20,\n",
    "    minibatch_size_features = 100,\n",
    "    save_dir = sae_save_dir,\n",
    "    data = dataset_config,\n",
    "    feature_indices = list(range(50)),\n",
    "    prompt_centric = PromptDashboardsConfig(\n",
    "        prompts = [\"Sally met Mike at the show. She brought popcorn for him.\"],\n",
    "        n_random_prompt_dashboards = 10\n",
    "    )\n",
    ")\n",
    "generate_dashboards(model, dashboards_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
