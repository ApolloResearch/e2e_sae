seed: 0
tlens_model_name: pythia-14m
tlens_model_path: null
train:
  save_every_n_samples: null
  n_samples: 15000
  batch_size: 12
  effective_batch_size: 24
  lr: 1e-3
  warmup_samples: 0
  max_grad_norm: 1.0
  loss_configs:
    sparsity:
      p_norm: 0.5
      coeff: 0.00005
    inp_to_orig: null
    out_to_orig:
      coeff: 1.0
    inp_to_out: null
    logits_kl: null
    normalize_mse: True
data:
  dataset_name: apollo-research/sae-monology-pile-uncopyrighted-tokenizer-EleutherAI-gpt-neox-20b
  is_tokenized: true
  tokenizer_name: EleutherAI/gpt-neox-20B
  streaming: true
  split: train
  n_ctx: 2048
saes:
  sae_position_names: blocks.3.hook_resid_post
  dict_size_to_input_ratio: 1.0
wandb_project: pythia-14m

